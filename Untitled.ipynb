{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c614133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/pegasus-large were not used when initializing PegasusForCausalLM: ['model.encoder.layers.8.fc1.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.13.fc1.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.15.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.15.fc2.bias', 'model.encoder.layers.15.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.13.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layer_norm.bias', 'model.encoder.layers.13.self_attn.k_proj.bias', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.13.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.14.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.14.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.12.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.14.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.14.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.14.self_attn.v_proj.weight', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.15.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.14.self_attn.q_proj.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.12.fc2.bias', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.15.fc2.weight', 'model.encoder.layers.13.fc2.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.15.fc1.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.12.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.14.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.12.final_layer_norm.weight', 'model.encoder.layers.15.self_attn.v_proj.bias', 'model.encoder.layers.13.self_attn.out_proj.weight', 'model.encoder.layer_norm.weight', 'model.encoder.layers.15.fc1.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.12.fc2.weight', 'model.encoder.layers.13.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.13.final_layer_norm.weight', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.12.self_attn.k_proj.bias', 'model.encoder.layers.12.final_layer_norm.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn_layer_norm.bias', 'model.encoder.layers.13.fc2.weight', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.13.self_attn.k_proj.weight', 'model.encoder.layers.12.self_attn.out_proj.weight', 'model.encoder.layers.12.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.12.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.12.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.12.fc1.bias', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.15.final_layer_norm.weight', 'model.encoder.layers.13.fc1.weight', 'model.encoder.layers.15.self_attn.q_proj.weight', 'model.encoder.layers.13.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn.q_proj.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.shared.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.12.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.12.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.14.self_attn.out_proj.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.12.fc1.weight', 'model.encoder.layers.15.self_attn_layer_norm.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.12.self_attn.q_proj.bias', 'model.encoder.layers.14.final_layer_norm.weight', 'model.encoder.layers.14.fc1.weight', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.14.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.13.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'final_logits_bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.14.fc2.weight', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.14.fc1.bias', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.14.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.13.final_layer_norm.bias', 'model.encoder.layers.15.self_attn.v_proj.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.15.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.15.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing PegasusForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForCausalLM were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)\n",
    "assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n",
    "list(logits.shape) == expected_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c28c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusTokenizerFast(name_or_path='google/pegasus-large', vocab_size=96103, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7667051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁restrain': 40070,\n",
       " '▁CASA': 57323,\n",
       " '▁rise': 2423,\n",
       " 'ENE': 36387,\n",
       " '▁shelve': 94391,\n",
       " '▁Totem': 72531,\n",
       " '▁Catalan': 28646,\n",
       " '▁Belong': 50703,\n",
       " '▁Very': 3695,\n",
       " '▁balmy': 60464,\n",
       " 'yaz': 72001,\n",
       " '▁foil': 8977,\n",
       " '▁Booker': 31856,\n",
       " '▁EJ': 41904,\n",
       " '▁whopping': 18525,\n",
       " 'BUS': 53547,\n",
       " '▁Artists': 13696,\n",
       " '▁Altar': 61914,\n",
       " 'broadcast': 63727,\n",
       " '▁reproducible': 50020,\n",
       " 'infidel': 90389,\n",
       " '▁Headmaster': 91044,\n",
       " '▁transmitted': 12184,\n",
       " 'kHz': 47723,\n",
       " 'hobo': 70692,\n",
       " '▁BSB': 86677,\n",
       " '▁graphic': 4133,\n",
       " 'she': 5686,\n",
       " '▁Serene': 70700,\n",
       " 'Evergreen': 88287,\n",
       " '▁Shao': 75218,\n",
       " '▁HEAVY': 88924,\n",
       " '▁encased': 43476,\n",
       " '▁Nigel': 25043,\n",
       " '▁Occasion': 37623,\n",
       " '▁mogul': 46546,\n",
       " 'oux': 44291,\n",
       " '▁garment': 13759,\n",
       " 'yep': 68073,\n",
       " '▁focal': 11072,\n",
       " '▁Arranged': 81712,\n",
       " '▁Thibodeau': 91169,\n",
       " '▁undecided': 45506,\n",
       " '▁occlus': 92532,\n",
       " '▁Containment': 95311,\n",
       " '▁SIMPLE': 70477,\n",
       " '▁AV': 14449,\n",
       " 'Ultimately': 21290,\n",
       " '▁riot': 25302,\n",
       " '▁persian': 66100,\n",
       " 'Jobs': 40533,\n",
       " '▁Nouveau': 44299,\n",
       " '▁snuggled': 68973,\n",
       " '▁Pell': 32829,\n",
       " '591': 46119,\n",
       " '▁consuming': 8045,\n",
       " '▁examine': 4973,\n",
       " '▁Genoa': 40054,\n",
       " '▁Considerations': 71907,\n",
       " '▁accounts': 2452,\n",
       " '▁superconducting': 91184,\n",
       " '▁burgeoning': 29672,\n",
       " '▁Conner': 41290,\n",
       " 'anti': 11561,\n",
       " '▁buttoned': 64533,\n",
       " '▁tablecloths': 54007,\n",
       " '▁bike': 2352,\n",
       " '▁practice': 846,\n",
       " '▁Loveland': 51824,\n",
       " '▁Ouija': 90113,\n",
       " 'blanc': 46625,\n",
       " 'FW': 31015,\n",
       " '▁polluting': 46425,\n",
       " 'SaaS': 66459,\n",
       " '▁underlie': 55872,\n",
       " '▁DROP': 59378,\n",
       " '▁corridors': 25799,\n",
       " 'lingual': 34096,\n",
       " 'Roger': 36958,\n",
       " '▁irreversible': 38483,\n",
       " \"▁'90\": 49708,\n",
       " 'bé': 47567,\n",
       " 'ician': 56423,\n",
       " 'CONNECT': 91022,\n",
       " 'Pitch': 59559,\n",
       " 'uff': 11527,\n",
       " '▁Pique': 83208,\n",
       " '▁exceeded': 12710,\n",
       " '▁sanitise': 86066,\n",
       " 'uter': 29933,\n",
       " '▁6000': 26128,\n",
       " '▁MOM': 61690,\n",
       " 'USS': 66984,\n",
       " '▁lightsaber': 64461,\n",
       " '▁Argyll': 76251,\n",
       " 'Roethlisberger': 80625,\n",
       " '▁Gerd': 80765,\n",
       " '▁bevy': 63024,\n",
       " '▁helpless': 26480,\n",
       " '▁Fishermen': 83668,\n",
       " 'odin': 85486,\n",
       " '▁meantime': 8798,\n",
       " 'relief': 61104,\n",
       " '▁Fusion': 15827,\n",
       " '▁Brief': 23954,\n",
       " '▁FRA': 40139,\n",
       " '▁(1984)': 64685,\n",
       " '▁Dear': 16930,\n",
       " 'lists': 29923,\n",
       " '▁Playground': 30039,\n",
       " '▁desiring': 46297,\n",
       " '▁Marie': 9262,\n",
       " '▁sacrifice': 7603,\n",
       " '▁clocks': 17898,\n",
       " '▁1.5-': 51047,\n",
       " '377': 36794,\n",
       " '▁detractors': 63749,\n",
       " '▁Awareness': 14971,\n",
       " '▁occ': 69338,\n",
       " '▁Sirius': 45968,\n",
       " '▁Bayley': 71242,\n",
       " 'BOARD': 72068,\n",
       " '▁waxes': 55503,\n",
       " '▁March': 1051,\n",
       " '0.5': 25884,\n",
       " 'jector': 72848,\n",
       " 'perception': 84505,\n",
       " 'opener': 65495,\n",
       " '▁Minimum': 16115,\n",
       " '▁excreted': 84386,\n",
       " '▁Hicks': 29578,\n",
       " 'elaar': 87229,\n",
       " '▁Teixeira': 89285,\n",
       " 'Pesa': 91769,\n",
       " '▁est': 14828,\n",
       " '▁Minaj': 53919,\n",
       " 'mal': 15067,\n",
       " '▁predetermined': 30409,\n",
       " 'khi': 43401,\n",
       " '▁Beyonce': 44107,\n",
       " '▁popsicle': 64256,\n",
       " '▁11.30': 63422,\n",
       " 'VCL': 84553,\n",
       " '▁Advocacy': 28135,\n",
       " '▁WARRANT': 85631,\n",
       " '▁Jordy': 92168,\n",
       " '▁Podiatrist': 92949,\n",
       " '▁Toulon': 93394,\n",
       " 'RE': 8961,\n",
       " 'pré': 60113,\n",
       " '▁Planck': 61873,\n",
       " '▁27%': 41220,\n",
       " '▁vi': 18604,\n",
       " 'existing': 19604,\n",
       " '268': 52827,\n",
       " '▁cognac': 56504,\n",
       " '▁probabilities': 38239,\n",
       " '▁Bum': 57661,\n",
       " '▁RSS': 7368,\n",
       " '▁Padilla': 77246,\n",
       " '845': 51015,\n",
       " '▁plyometric': 84314,\n",
       " '▁Guitarist': 80883,\n",
       " 'Comprehensive': 47826,\n",
       " '▁suburban': 15578,\n",
       " 'Oz': 58043,\n",
       " 'tera': 30875,\n",
       " '▁Corner': 10330,\n",
       " '▁impose': 14281,\n",
       " '▁untangle': 82628,\n",
       " '▁Oncology': 28662,\n",
       " '▁stiffer': 49822,\n",
       " '▁96': 9048,\n",
       " '▁father': 1802,\n",
       " '▁Slowly': 29022,\n",
       " '▁mating': 36744,\n",
       " 'CX': 38142,\n",
       " '▁evasion': 41729,\n",
       " '▁Contour': 46202,\n",
       " '▁Beneath': 53837,\n",
       " '▁agrees': 13174,\n",
       " 'hic': 64780,\n",
       " '▁Pola': 70834,\n",
       " 'NAB': 78770,\n",
       " 'PAW': 70230,\n",
       " '▁7,': 6691,\n",
       " '▁Ambassador': 12931,\n",
       " '▁Rajasthan': 20462,\n",
       " '▁Drones': 64930,\n",
       " '▁saxophonist': 52355,\n",
       " 'On': 1189,\n",
       " '▁hits': 4341,\n",
       " '▁trade': 1456,\n",
       " '▁W': 1354,\n",
       " '▁laser': 4344,\n",
       " '▁Fell': 39765,\n",
       " '▁probabilistic': 65108,\n",
       " '▁mislead': 46983,\n",
       " 'fixing': 67070,\n",
       " 'guine': 80210,\n",
       " 'interesting': 40096,\n",
       " '▁Horus': 81080,\n",
       " '▁circadian': 55435,\n",
       " '▁Esso': 92439,\n",
       " '▁Puck': 45596,\n",
       " 'wy': 19717,\n",
       " '▁nu': 22858,\n",
       " '▁sparks': 31378,\n",
       " 'strange': 63952,\n",
       " '▁epilogue': 90097,\n",
       " 'ficent': 63442,\n",
       " '▁JET': 62834,\n",
       " 'pf': 22594,\n",
       " '▁tightening': 22059,\n",
       " 'dian': 24382,\n",
       " '▁worthless': 26809,\n",
       " '▁ESL': 28339,\n",
       " '▁mischievous': 42807,\n",
       " '▁ALD': 68327,\n",
       " '▁spoil': 19106,\n",
       " 'LTA': 43737,\n",
       " 'Yea': 55460,\n",
       " '▁sera': 68669,\n",
       " '▁Hard': 6730,\n",
       " '▁PES': 40796,\n",
       " 'grilled': 71543,\n",
       " '▁Mee': 55233,\n",
       " '▁JBoss': 74948,\n",
       " 'bugs': 48848,\n",
       " '▁Receiving': 51034,\n",
       " '▁Evangelism': 81039,\n",
       " '▁exclusion': 15925,\n",
       " '4.8%': 74994,\n",
       " '▁archers': 71218,\n",
       " 'wo': 13928,\n",
       " 'elf': 40622,\n",
       " '▁wriggle': 86429,\n",
       " '▁removal': 3083,\n",
       " '▁wettest': 79417,\n",
       " '555': 38293,\n",
       " '▁Entertain': 85963,\n",
       " '▁allowing': 2063,\n",
       " '▁Pyramid': 27918,\n",
       " 'THERM': 83709,\n",
       " '▁illegible': 86404,\n",
       " '▁invests': 33414,\n",
       " '▁IMPROVE': 84993,\n",
       " '▁Leger': 86675,\n",
       " '▁Rabi': 55505,\n",
       " 'rist': 20119,\n",
       " 'TIG': 79595,\n",
       " '▁underlayment': 86085,\n",
       " 'PAGE': 67787,\n",
       " '▁FTD': 71509,\n",
       " '▁Zynga': 67414,\n",
       " '▁Cypriot': 67142,\n",
       " '▁Cyr': 70713,\n",
       " 'though': 11264,\n",
       " '▁Nashua': 72160,\n",
       " '▁borehole': 76974,\n",
       " '▁midstream': 83362,\n",
       " '▁rarest': 49934,\n",
       " 'Interest': 42234,\n",
       " '▁bucking': 80251,\n",
       " '▁Welded': 88765,\n",
       " '▁(1961)': 89359,\n",
       " 'oiled': 44687,\n",
       " '▁Shih': 53690,\n",
       " 'ordered': 30543,\n",
       " '▁playable': 27233,\n",
       " '▁lemur': 86880,\n",
       " '▁cranberries': 32207,\n",
       " '▁oversize': 50119,\n",
       " '2:05': 63028,\n",
       " '▁registrar': 27293,\n",
       " '▁apt': 18113,\n",
       " '▁Keeper': 45753,\n",
       " '▁conjunct': 73931,\n",
       " '▁PROPERTY': 56855,\n",
       " 'indicate': 60826,\n",
       " '7-17': 81493,\n",
       " '▁creditor': 26340,\n",
       " 'lana': 35564,\n",
       " 'Violent': 95387,\n",
       " '▁Duluth': 39350,\n",
       " '▁WMU': 81237,\n",
       " '▁25-': 14827,\n",
       " '▁persisted': 38017,\n",
       " 'Ladies': 46705,\n",
       " '▁Duterte': 44098,\n",
       " '▁zone': 3570,\n",
       " '▁Ri': 18665,\n",
       " 'Cannabidiol': 90768,\n",
       " '▁Alert': 17504,\n",
       " '2.16': 76154,\n",
       " '▁diffusing': 90879,\n",
       " 'Vin': 52612,\n",
       " '▁harshly': 55764,\n",
       " '▁gif': 42987,\n",
       " '▁Casablanca': 56318,\n",
       " '▁depiction': 29249,\n",
       " '▁Brum': 51363,\n",
       " '▁Tad': 33402,\n",
       " '▁Sema': 66525,\n",
       " '▁Coffin': 70554,\n",
       " '▁stalagmite': 85667,\n",
       " 'fertilize': 59609,\n",
       " '▁judge': 3987,\n",
       " 'mane': 61092,\n",
       " '▁Rihanna': 37056,\n",
       " 'mira': 41691,\n",
       " '▁Tailored': 65647,\n",
       " '▁Salk': 86652,\n",
       " '▁Powertrain': 86898,\n",
       " 'Herbal': 90311,\n",
       " '▁Armadillo': 91533,\n",
       " '▁NRC': 38331,\n",
       " '▁Manuals': 29956,\n",
       " '▁Subjects': 45891,\n",
       " '▁exalt': 69829,\n",
       " 'ana': 5181,\n",
       " '▁marital': 23188,\n",
       " 'Unscrew': 93515,\n",
       " 'ated': 8389,\n",
       " '▁modes': 8190,\n",
       " '▁innovations': 8601,\n",
       " '945': 57165,\n",
       " 'Enfant': 76623,\n",
       " '▁Liebherr': 86012,\n",
       " '▁2030': 23807,\n",
       " 'hobby': 81473,\n",
       " 'did': 19706,\n",
       " 'cn': 22731,\n",
       " 'suited': 35617,\n",
       " '▁Svalbard': 72393,\n",
       " '▁MEAN': 62304,\n",
       " '▁Mato': 62618,\n",
       " '▁reassurance': 27758,\n",
       " 'eet': 57190,\n",
       " 'Out': 10112,\n",
       " '▁Vegan': 21264,\n",
       " '▁Kalyan': 61241,\n",
       " '▁cad': 50244,\n",
       " '▁silliness': 64890,\n",
       " 'Actively': 73296,\n",
       " '▁Til': 29360,\n",
       " 'yourself': 27213,\n",
       " '▁(90': 38738,\n",
       " '-93': 48338,\n",
       " 'icide': 32286,\n",
       " '▁(1986)': 62499,\n",
       " '▁NMD': 67781,\n",
       " '▁COAST': 86917,\n",
       " '▁blindfold': 92421,\n",
       " 'ido': 15398,\n",
       " 'ock': 16242,\n",
       " '▁Cartridge': 32720,\n",
       " 'Damn': 57691,\n",
       " 'STIC': 62416,\n",
       " 'RIL': 87368,\n",
       " '592': 54090,\n",
       " '▁locations': 2081,\n",
       " 'xlsx': 94952,\n",
       " 'Stomach': 68126,\n",
       " '▁Schenectady': 79642,\n",
       " '▁verb': 16448,\n",
       " '▁whimsical': 21797,\n",
       " 'payments': 43636,\n",
       " '▁naked': 14884,\n",
       " '▁5-6': 23867,\n",
       " 'Hawaii': 48526,\n",
       " '▁recruiters': 21706,\n",
       " '▁peacefully': 21776,\n",
       " '▁transcripts': 25425,\n",
       " '▁speak': 1799,\n",
       " '003': 25767,\n",
       " '▁struggle': 3718,\n",
       " '▁Anyway': 11331,\n",
       " '▁tangy': 27328,\n",
       " 'mino': 46592,\n",
       " '▁Jamestown': 42549,\n",
       " '▁genomes': 56083,\n",
       " '▁Attorneys': 30358,\n",
       " '▁Downloading': 69053,\n",
       " '▁utilise': 21148,\n",
       " 'enger': 46287,\n",
       " '▁Koro': 57486,\n",
       " '▁immobile': 79381,\n",
       " '▁Sash': 57520,\n",
       " '▁Thyme': 67045,\n",
       " '▁Folio': 67754,\n",
       " '▁Watchtower': 88632,\n",
       " '▁CarRentals': 91579,\n",
       " '▁Indi': 47871,\n",
       " '▁list': 467,\n",
       " '▁battleship': 64157,\n",
       " 'db': 17416,\n",
       " '▁improved': 2521,\n",
       " 'ush': 20649,\n",
       " 'nad': 33845,\n",
       " '▁serving': 2036,\n",
       " '▁Natick': 91272,\n",
       " '▁QUART': 95238,\n",
       " '▁Headingley': 94580,\n",
       " 'CSR': 53328,\n",
       " '▁Installers': 66784,\n",
       " '▁Duh': 81057,\n",
       " 'rare': 42483,\n",
       " '▁Leiden': 53111,\n",
       " 'ethanol': 57285,\n",
       " 'Bespoke': 93565,\n",
       " '▁Spaces': 30714,\n",
       " '▁lake': 3971,\n",
       " '▁squirt': 45178,\n",
       " '▁campaigners': 50669,\n",
       " '▁ovation': 44501,\n",
       " '▁Suzy': 65853,\n",
       " '▁Cuisinart': 82551,\n",
       " 'TCH': 49097,\n",
       " '▁15:3': 84556,\n",
       " '▁Bluff': 37679,\n",
       " '▁hypothesized': 60613,\n",
       " 'lé': 22681,\n",
       " '▁dramatiz': 76497,\n",
       " 'EME': 52996,\n",
       " '▁23:38:3': 92310,\n",
       " 'PSE': 70842,\n",
       " 'ATE': 22153,\n",
       " '▁smearing': 85836,\n",
       " 'sac': 23880,\n",
       " 'Injuries': 93200,\n",
       " '▁Leisure': 22490,\n",
       " '▁introductory': 14600,\n",
       " '▁eg': 24117,\n",
       " 'jung': 39026,\n",
       " '▁guitarist': 15275,\n",
       " '▁Lipstick': 46205,\n",
       " '▁lamented': 49238,\n",
       " 'Symptoms': 63055,\n",
       " '▁deleted': 8329,\n",
       " '▁LPGA': 73291,\n",
       " '▁Rowling': 42728,\n",
       " '▁spaceflight': 70959,\n",
       " '▁DPW': 93156,\n",
       " '▁agencies': 2730,\n",
       " '▁downside': 13587,\n",
       " '▁bum': 32402,\n",
       " '▁assimilation': 49600,\n",
       " '5:29': 94252,\n",
       " 'Shred': 78240,\n",
       " 'PI': 15844,\n",
       " 'witt': 49123,\n",
       " '▁Ser': 14654,\n",
       " '▁cloudy': 14274,\n",
       " '▁$62': 61800,\n",
       " 'chten': 63466,\n",
       " 'germain': 95707,\n",
       " 'oose': 44951,\n",
       " 'Sell': 35684,\n",
       " '▁Night': 4028,\n",
       " '▁hyped': 45363,\n",
       " 'expense': 82126,\n",
       " '▁centurion': 94838,\n",
       " '▁resection': 58953,\n",
       " '▁Gym': 16184,\n",
       " '▁jerseys': 15515,\n",
       " '▁scouting': 27726,\n",
       " '▁Llan': 37238,\n",
       " '▁browned': 20984,\n",
       " '▁Conditioners': 23396,\n",
       " 'Coffee': 31346,\n",
       " 'uche': 33824,\n",
       " '▁entryway': 23140,\n",
       " '▁journalistic': 39842,\n",
       " '▁1818': 58162,\n",
       " '▁cards': 1519,\n",
       " 'Every': 4334,\n",
       " 'yacht': 67326,\n",
       " '▁MONSTER': 93380,\n",
       " 'vit': 18823,\n",
       " 'ETC': 79482,\n",
       " 'Ohhh': 86983,\n",
       " 'Sales': 22301,\n",
       " '▁DIT': 82034,\n",
       " '▁iron': 2889,\n",
       " 'building': 9659,\n",
       " '▁lob': 52866,\n",
       " 'Captain': 31552,\n",
       " '▁repainting': 63379,\n",
       " '▁phenomenal': 13996,\n",
       " '965': 46610,\n",
       " 'MES': 34270,\n",
       " '▁tulip': 54300,\n",
       " '▁kde': 75776,\n",
       " 'portable': 42769,\n",
       " '▁Select': 5535,\n",
       " '▁sucrose': 74900,\n",
       " '▁commanding': 25447,\n",
       " '▁1994.': 20465,\n",
       " '▁fallow': 76097,\n",
       " 'TK': 31784,\n",
       " '▁numerous': 1866,\n",
       " '▁protection': 1471,\n",
       " '▁predictability': 54393,\n",
       " '▁republican': 47785,\n",
       " 'ower': 66234,\n",
       " '▁Williamsburg': 33308,\n",
       " '▁Storm': 9442,\n",
       " '▁reviewer': 22356,\n",
       " 'tow': 37180,\n",
       " 'paul': 44054,\n",
       " 'chemistry': 36204,\n",
       " '▁ZDNet': 95216,\n",
       " '▁Dinh': 84433,\n",
       " '▁LOUD': 91995,\n",
       " '▁different': 291,\n",
       " '▁Territories': 40899,\n",
       " '▁Carlyle': 56173,\n",
       " '▁apples': 9098,\n",
       " '▁Westland': 77603,\n",
       " '▁Wilshire': 89004,\n",
       " '▁Improving': 37201,\n",
       " '▁feathered': 52933,\n",
       " '▁PAINT': 80164,\n",
       " '▁Tidal': 67206,\n",
       " '▁$18': 19374,\n",
       " 'federal': 70249,\n",
       " '▁MACH': 87381,\n",
       " 'Items': 36291,\n",
       " '▁CHALLENGE': 69833,\n",
       " '▁>>': 17578,\n",
       " 'prefix': 86216,\n",
       " '▁garden': 1484,\n",
       " 'hla': 50570,\n",
       " '▁prostrate': 86502,\n",
       " '▁Hayes': 18157,\n",
       " 'cept': 22554,\n",
       " '▁anyways': 30076,\n",
       " 'Mar': 10510,\n",
       " '▁hospitality': 7191,\n",
       " '▁Abs': 35088,\n",
       " 'catcher': 40368,\n",
       " '▁avg': 74759,\n",
       " 'Rotary': 88688,\n",
       " '979': 60728,\n",
       " '▁china': 10161,\n",
       " '▁occurring': 9102,\n",
       " '▁tarnish': 41073,\n",
       " 'Germany': 23914,\n",
       " '1.75': 78613,\n",
       " '▁oddball': 74693,\n",
       " '▁Horton': 30192,\n",
       " 'could': 24517,\n",
       " '▁tailored': 5516,\n",
       " '▁whitening': 15881,\n",
       " '▁plans': 1017,\n",
       " '▁Cotswold': 73356,\n",
       " '▁Ema': 68009,\n",
       " '▁Cheating': 77380,\n",
       " '▁orchestration': 42229,\n",
       " '▁enjoin': 78143,\n",
       " '▁Safa': 63887,\n",
       " 'anne': 16826,\n",
       " 'Rough': 80517,\n",
       " '▁blitz': 36549,\n",
       " '▁UHS': 81138,\n",
       " '▁Flathead': 70928,\n",
       " '▁Weldon': 85891,\n",
       " '▁Indica': 55205,\n",
       " '▁Unless': 11138,\n",
       " 'Courses': 60035,\n",
       " '946': 64937,\n",
       " '▁processes': 1994,\n",
       " '▁Ravelry': 74618,\n",
       " '▁10:10': 85185,\n",
       " 'tta': 16460,\n",
       " '▁former': 1319,\n",
       " '▁curls': 25235,\n",
       " '▁finds': 5258,\n",
       " '▁freely': 7449,\n",
       " 'brough': 51125,\n",
       " '▁desires': 8359,\n",
       " '▁summon': 31676,\n",
       " 'homi': 82707,\n",
       " 'Ready': 15402,\n",
       " '▁1816': 69313,\n",
       " '▁spoilt': 38273,\n",
       " '▁Amazing': 8306,\n",
       " '▁fallback': 69932,\n",
       " '▁Gro': 13824,\n",
       " '▁uphold': 18396,\n",
       " 'blue': 11753,\n",
       " '▁futurist': 72801,\n",
       " '▁Worst': 32567,\n",
       " '▁CARD': 46044,\n",
       " '▁unselfish': 66034,\n",
       " 'Street': 24605,\n",
       " '▁DTS': 55866,\n",
       " '▁BI': 14234,\n",
       " '▁sacrificed': 28842,\n",
       " '▁1600': 16294,\n",
       " '▁Sconce': 50492,\n",
       " '▁Yahoo': 10025,\n",
       " '▁grandfather': 10517,\n",
       " 'tons': 40927,\n",
       " '▁Clouds': 50986,\n",
       " '▁Therapy': 7802,\n",
       " 'using': 12093,\n",
       " '▁1869': 52775,\n",
       " '▁Preview': 20059,\n",
       " '▁Suits': 51837,\n",
       " '▁Avent': 61543,\n",
       " '▁posts': 2131,\n",
       " '▁Danny': 12379,\n",
       " '▁iced': 23264,\n",
       " '▁400-': 46991,\n",
       " '▁164': 34996,\n",
       " '▁thrice': 50495,\n",
       " '▁compositional': 57460,\n",
       " '45%': 62840,\n",
       " '▁PTFE': 71371,\n",
       " '▁syndicate': 43409,\n",
       " '▁earmarked': 48026,\n",
       " '▁Twilio': 79672,\n",
       " '▁permissive': 82646,\n",
       " 'ABLE': 17977,\n",
       " '▁JAW': 87794,\n",
       " 'school': 6800,\n",
       " 'percentage': 93589,\n",
       " 'ection': 57853,\n",
       " '▁Translucent': 83283,\n",
       " 'pool': 15565,\n",
       " '▁secession': 70627,\n",
       " '▁Prosecution': 62885,\n",
       " '▁spider': 14097,\n",
       " '▁undemocratic': 87926,\n",
       " 'lock': 8691,\n",
       " 'Sc': 16053,\n",
       " '▁dictionaries': 48413,\n",
       " '▁napkins': 31949,\n",
       " '▁sensual': 34403,\n",
       " '▁unilaterally': 59490,\n",
       " '▁FFA': 51977,\n",
       " '75': 10087,\n",
       " '▁chopper': 37713,\n",
       " '▁identifier': 22225,\n",
       " '▁alchemy': 58482,\n",
       " '▁Nomura': 82354,\n",
       " '▁checkboxes': 80086,\n",
       " '▁2014-15': 42902,\n",
       " '▁swore': 50627,\n",
       " '▁Waltham': 48701,\n",
       " '▁Lovecraft': 54408,\n",
       " '▁roller': 7369,\n",
       " '▁movable': 36412,\n",
       " '▁Mayweather': 53132,\n",
       " '▁accidents': 6176,\n",
       " '▁Bennett': 14700,\n",
       " '1950': 60884,\n",
       " 'greet': 69260,\n",
       " '▁Vocalist': 82031,\n",
       " '▁Derecho': 95265,\n",
       " '▁Royal': 3084,\n",
       " '▁paprika': 31865,\n",
       " '▁Rakesh': 90122,\n",
       " '▁Fermanagh': 92201,\n",
       " '▁expendable': 78153,\n",
       " '7200': 56353,\n",
       " '▁commencing': 31091,\n",
       " 'Brussels': 90616,\n",
       " '▁hitherto': 53412,\n",
       " '▁Subscribe': 19985,\n",
       " '▁Ecclesia': 88868,\n",
       " 'Mad': 24541,\n",
       " 'Fi': 4496,\n",
       " 'Vivi': 94257,\n",
       " '▁statewide': 14258,\n",
       " 'neurysm': 94382,\n",
       " 'busters': 52841,\n",
       " '▁Tract': 56952,\n",
       " '▁bother': 9194,\n",
       " 'H': 1587,\n",
       " '-7': 11581,\n",
       " '▁Shaykh': 93391,\n",
       " '▁Lecturer': 30151,\n",
       " 'teborg': 92780,\n",
       " 'lmaz': 95424,\n",
       " '▁Trudy': 95693,\n",
       " '▁Compartment': 94473,\n",
       " '▁Weaving': 67417,\n",
       " '▁Coachella': 43187,\n",
       " '▁sur': 14455,\n",
       " '▁Subs': 54897,\n",
       " 'Admittedly': 67255,\n",
       " 'oglio': 74172,\n",
       " '▁Minor': 16350,\n",
       " 'megawatt': 55457,\n",
       " '▁180': 7482,\n",
       " 'HFA': 65477,\n",
       " 'intrusive': 86330,\n",
       " '▁Jeep': 11411,\n",
       " '▁coalition': 10382,\n",
       " 'Loo': 71215,\n",
       " '▁Koi': 46165,\n",
       " '1.00': 53001,\n",
       " '▁decried': 85001,\n",
       " 'nl': 20123,\n",
       " 'ines': 21734,\n",
       " '▁henceforth': 77537,\n",
       " 'fed': 18731,\n",
       " '▁federation': 26507,\n",
       " '▁$49.95': 90504,\n",
       " '▁Yoshi': 30087,\n",
       " 'ppy': 33513,\n",
       " '▁Striped': 53532,\n",
       " 'tima': 47813,\n",
       " '5-8': 42775,\n",
       " '▁Civic': 14614,\n",
       " '▁Incomplete': 79860,\n",
       " '▁Manson': 58030,\n",
       " '▁ending': 4439,\n",
       " '▁Linder': 85205,\n",
       " 'ebe': 41991,\n",
       " '▁Kilimanjaro': 51401,\n",
       " '▁Bani': 71250,\n",
       " '▁Agio': 72093,\n",
       " '▁buddha': 81317,\n",
       " '▁Deen': 65527,\n",
       " '▁calibrating': 87489,\n",
       " '▁Martinique': 64012,\n",
       " '▁Gymnastics': 46185,\n",
       " 'Hali': 74142,\n",
       " '▁Having': 3694,\n",
       " '▁Shaped': 51513,\n",
       " '879': 54168,\n",
       " 'UME': 69492,\n",
       " '▁Shingles': 62351,\n",
       " 'Requirements': 74912,\n",
       " '▁Sek': 56356,\n",
       " 'actuated': 87518,\n",
       " '▁recede': 61574,\n",
       " '▁Sclerosis': 67149,\n",
       " '▁psychological': 6804,\n",
       " 'part': 8316,\n",
       " '▁Ana': 10506,\n",
       " '▁dusk': 25388,\n",
       " '▁proposes': 19390,\n",
       " '▁justification': 19056,\n",
       " '▁Fritz': 43794,\n",
       " 'dade': 59723,\n",
       " '▁insiders': 32908,\n",
       " '▁CABINET': 94420,\n",
       " '▁Colours': 39283,\n",
       " '▁Johnstown': 82683,\n",
       " '▁Stru': 49569,\n",
       " '▁mummies': 76915,\n",
       " '▁Searching': 26397,\n",
       " '▁Stitch': 29558,\n",
       " 'ade': 9259,\n",
       " '▁pantries': 58498,\n",
       " '▁fusing': 48170,\n",
       " 'cats': 44203,\n",
       " '▁nifty': 28981,\n",
       " '▁Amin': 51322,\n",
       " '▁guaranteed': 3891,\n",
       " '▁28-30': 89212,\n",
       " 'embl': 61467,\n",
       " '▁ridden': 27576,\n",
       " 'Equipment': 46951,\n",
       " '▁WebMD': 69428,\n",
       " '▁favourably': 74362,\n",
       " '▁blinking': 38881,\n",
       " 'aug': 37673,\n",
       " 'Legitimate': 83597,\n",
       " 'tw': 30191,\n",
       " 'Acquire': 70726,\n",
       " '▁Haitian': 33688,\n",
       " '▁Jeri': 78677,\n",
       " '▁Drone': 33689,\n",
       " '▁strapping': 59486,\n",
       " 'arissa': 57194,\n",
       " 'Nash': 69399,\n",
       " 'poster': 56592,\n",
       " 'Bureau': 95416,\n",
       " '▁toppings': 20788,\n",
       " '▁Admittedly': 50725,\n",
       " '▁appetizing': 61820,\n",
       " 'pler': 79861,\n",
       " '▁Bloch': 86950,\n",
       " '▁BOAT': 84122,\n",
       " '▁Esther': 29656,\n",
       " '▁Fluency': 93395,\n",
       " 'Bean': 51601,\n",
       " '▁ruthlessly': 78807,\n",
       " '▁porcupine': 86029,\n",
       " '▁physicists': 42919,\n",
       " 'PEC': 49247,\n",
       " '▁demonize': 86913,\n",
       " 'wish': 26882,\n",
       " '▁Learning': 4473,\n",
       " '▁glad': 2857,\n",
       " 'Select': 9253,\n",
       " 'sweet': 21070,\n",
       " 'WAN': 29696,\n",
       " '▁ribbed': 36961,\n",
       " 'dang': 46306,\n",
       " '▁exemplar': 62872,\n",
       " '▁notwithstanding': 33244,\n",
       " '▁stainless': 3823,\n",
       " '▁rockstar': 66895,\n",
       " '▁$0.99': 77844,\n",
       " 'arian': 23338,\n",
       " '▁FTP': 22662,\n",
       " 'Canadian': 25829,\n",
       " '▁Establish': 43941,\n",
       " 'nasa': 83498,\n",
       " '▁Capella': 92822,\n",
       " '▁Rapha': 93260,\n",
       " '▁woeful': 95139,\n",
       " '▁Eugenio': 95878,\n",
       " '1953': 73621,\n",
       " '▁Mir': 18229,\n",
       " 'TRON': 57523,\n",
       " '▁saffron': 33975,\n",
       " '▁ABA': 30464,\n",
       " '▁Ribbon': 22668,\n",
       " 'jek': 50230,\n",
       " 'Courtesy': 69762,\n",
       " 'ostasis': 86207,\n",
       " '▁Doreen': 75229,\n",
       " '▁evaporates': 67851,\n",
       " '▁bishop': 24281,\n",
       " '▁Fogg': 66379,\n",
       " 'suffering': 74916,\n",
       " '▁versions': 3328,\n",
       " '▁absorbing': 19666,\n",
       " '▁WIP': 48658,\n",
       " '▁Palmdale': 89053,\n",
       " 'vata': 72050,\n",
       " '▁held': 886,\n",
       " 'azz': 38200,\n",
       " 'YD': 45158,\n",
       " 'obtain': 82295,\n",
       " '▁Whitefish': 87682,\n",
       " 'configured': 49917,\n",
       " '▁Caledonia': 53394,\n",
       " '▁VR': 8673,\n",
       " '10-7': 84896,\n",
       " '▁Ronald': 16335,\n",
       " '▁inaction': 51451,\n",
       " '▁lick': 37611,\n",
       " '▁padded': 15979,\n",
       " '115': 23681,\n",
       " '▁hcg': 51101,\n",
       " '▁pretended': 54851,\n",
       " 'uation': 47245,\n",
       " 'yles': 72548,\n",
       " '▁Maja': 77082,\n",
       " '▁PAYE': 83398,\n",
       " 'impson': 85745,\n",
       " '▁Containers': 48059,\n",
       " '▁Mozzarella': 82878,\n",
       " '▁silk': 7715,\n",
       " '▁Buhari': 26250,\n",
       " 'Ad': 17867,\n",
       " '▁procrastinator': 83933,\n",
       " '▁ensuing': 28896,\n",
       " '▁Christina': 20090,\n",
       " '▁approximation': 34572,\n",
       " '▁grapple': 34795,\n",
       " '▁bachelors': 57870,\n",
       " '▁ratio': 4641,\n",
       " '▁Dec': 4736,\n",
       " '▁unfolding': 26230,\n",
       " '▁Sul': 35765,\n",
       " '▁optimizer': 73440,\n",
       " 'escu': 35855,\n",
       " '▁Shiel': 93296,\n",
       " '▁state': 449,\n",
       " '▁Benji': 84220,\n",
       " '▁Radio': 4474,\n",
       " '▁assassinated': 55124,\n",
       " 'compliant': 29497,\n",
       " 'vista': 35151,\n",
       " '▁1976': 18913,\n",
       " '▁recalled': 14062,\n",
       " 'Registr': 61956,\n",
       " '▁EBS': 66223,\n",
       " '▁pranayama': 79659,\n",
       " '▁Jihad': 52272,\n",
       " '▁truly': 1388,\n",
       " '▁Skyscanner': 56422,\n",
       " 'Detach': 80910,\n",
       " 'Hang': 45122,\n",
       " '▁Schla': 55943,\n",
       " '▁honeymoon': 16542,\n",
       " '▁itinerary': 12988,\n",
       " 'allo': 38536,\n",
       " 'ilita': 82178,\n",
       " '▁hulking': 83672,\n",
       " '▁unfunded': 85785,\n",
       " '▁Solver': 88122,\n",
       " 'Inspiring': 88156,\n",
       " '▁Tamara': 39097,\n",
       " '▁pebbles': 43968,\n",
       " 'braid': 89537,\n",
       " '▁philosophy': 4679,\n",
       " 'ER': 6058,\n",
       " '▁Norman': 11362,\n",
       " '▁sexually': 23137,\n",
       " '▁Git': 29094,\n",
       " '▁honorable': 28421,\n",
       " '▁Famer': 51276,\n",
       " '▁Liner': 44895,\n",
       " '▁decide': 1854,\n",
       " '▁inquest': 60737,\n",
       " '▁lunches': 18032,\n",
       " '▁his': 169,\n",
       " '▁dents': 44076,\n",
       " 'baba': 61285,\n",
       " '▁promotional': 6082,\n",
       " '▁circuit': 4831,\n",
       " 'bach': 14189,\n",
       " '▁tactics': 8327,\n",
       " '▁.....': 26528,\n",
       " '▁EVs': 54012,\n",
       " '▁TTL': 71005,\n",
       " 'khov': 78266,\n",
       " '▁ambush': 50959,\n",
       " '▁Indexes': 84140,\n",
       " '▁panhandle': 87366,\n",
       " '2:23': 85559,\n",
       " '13-6': 83221,\n",
       " '▁Aviation': 12792,\n",
       " '▁pyrolysis': 89325,\n",
       " '▁trample': 92141,\n",
       " '▁truncate': 92585,\n",
       " 'Medline': 72539,\n",
       " '-1960': 64562,\n",
       " 'yet': 19287,\n",
       " '▁Claims': 21753,\n",
       " '▁metaphysics': 72950,\n",
       " '▁Difficult': 38421,\n",
       " '▁Luca': 32954,\n",
       " 'Gil': 47807,\n",
       " '▁faxing': 57575,\n",
       " '▁omnipresent': 65808,\n",
       " '▁Markdown': 74467,\n",
       " '▁Opener': 50376,\n",
       " '▁Singer': 20549,\n",
       " '▁Vibe': 36618,\n",
       " '▁Santa': 3226,\n",
       " '▁Homepage': 52598,\n",
       " '▁numerical': 17327,\n",
       " '▁haunting': 22098,\n",
       " '▁bulimia': 82886,\n",
       " '▁Daz': 74792,\n",
       " '▁materialized': 61852,\n",
       " '▁Meena': 83753,\n",
       " 'vimeo': 91584,\n",
       " '▁letter': 1801,\n",
       " '587': 45869,\n",
       " '▁Makerspace': 94597,\n",
       " '▁antidote': 39502,\n",
       " '▁sponsorships': 42608,\n",
       " '▁Voodoo': 60170,\n",
       " '▁flavourful': 64985,\n",
       " '▁consulate': 38487,\n",
       " 'Jacques': 72893,\n",
       " '▁angola': 92036,\n",
       " '▁Fun': 7122,\n",
       " 'Porn': 88137,\n",
       " '▁paint': 1999,\n",
       " '▁emphasize': 12731,\n",
       " '▁thoroughness': 77944,\n",
       " '▁Smash': 24816,\n",
       " '▁Kada': 78863,\n",
       " '▁Bridgwater': 91933,\n",
       " '▁materialism': 64832,\n",
       " '▁FB': 15721,\n",
       " 'FFFFFF': 95476,\n",
       " 'grading': 59555,\n",
       " '▁methods': 1625,\n",
       " '▁Expression': 30011,\n",
       " 'THO': 77964,\n",
       " '▁thundering': 81136,\n",
       " '▁Bunn': 82504,\n",
       " 'Morrow': 95820,\n",
       " '▁banquet': 18584,\n",
       " '▁inhale': 33048,\n",
       " '▁strobe': 48422,\n",
       " '▁tokens': 14426,\n",
       " '▁sterilise': 88956,\n",
       " '▁maverick': 62031,\n",
       " 'three': 12207,\n",
       " 'towel': 70234,\n",
       " '▁Igloo': 95257,\n",
       " 'Price': 14016,\n",
       " '▁loudspeakers': 62464,\n",
       " 'uren': 64615,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9671f144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusForCausalLM(\n",
       "  (model): PegasusDecoderWrapper(\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23c8f35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4287f552bf23424187fbac7cd248fc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64c0f14f788463e9ff82ce9f14fad94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"spiece.model\";:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7621dc111d54b248d1a4d4a4c472286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f1ddbf3921420d92e71a09cb144c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2fc5e3c5604b0abe6cdc8e6542e743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type pegasus_x to instantiate a model of type pegasus. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb36e90f33b489eb073cb9ec15a57d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/pegasus-x-large were not used when initializing PegasusModel: ['model.encoder.layers.9.global_self_attn_layer_norm.weight', 'model.encoder.layers.1.global_self_attn_layer_norm.bias', 'model.encoder.layers.12.global_self_attn_layer_norm.bias', 'model.encoder.layers.12.global_self_attn_layer_norm.weight', 'model.encoder.layers.13.global_self_attn_layer_norm.weight', 'model.encoder.layers.0.global_self_attn_layer_norm.weight', 'lm_head.weight', 'model.encoder.layers.13.global_self_attn_layer_norm.bias', 'model.encoder.layers.10.global_self_attn_layer_norm.weight', 'model.encoder.layers.11.global_self_attn_layer_norm.weight', 'model.encoder.layers.5.global_self_attn_layer_norm.weight', 'model.encoder.layers.14.global_self_attn_layer_norm.weight', 'model.encoder.layers.3.global_self_attn_layer_norm.bias', 'model.encoder.layers.8.global_self_attn_layer_norm.weight', 'model.encoder.embed_global.weight', 'model.encoder.layers.15.global_self_attn_layer_norm.weight', 'model.encoder.layers.7.global_self_attn_layer_norm.bias', 'model.encoder.layers.2.global_self_attn_layer_norm.bias', 'model.encoder.layers.5.global_self_attn_layer_norm.bias', 'model.encoder.layers.3.global_self_attn_layer_norm.weight', 'model.encoder.layers.9.global_self_attn_layer_norm.bias', 'model.encoder.layers.4.global_self_attn_layer_norm.weight', 'model.encoder.layers.6.global_self_attn_layer_norm.weight', 'model.encoder.layers.8.global_self_attn_layer_norm.bias', 'model.encoder.layers.7.global_self_attn_layer_norm.weight', 'model.encoder.layers.0.global_self_attn_layer_norm.bias', 'model.encoder.layers.4.global_self_attn_layer_norm.bias', 'model.encoder.layers.15.global_self_attn_layer_norm.bias', 'model.encoder.layers.10.global_self_attn_layer_norm.bias', 'model.encoder.layers.6.global_self_attn_layer_norm.bias', 'model.encoder.layers.14.global_self_attn_layer_norm.bias', 'model.encoder.layers.2.global_self_attn_layer_norm.weight', 'model.encoder.layers.1.global_self_attn_layer_norm.weight', 'model.encoder.layers.11.global_self_attn_layer_norm.bias']\n",
      "- This IS expected if you are initializing PegasusModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PegasusModel were not initialized from the model checkpoint at google/pegasus-x-large and are newly initialized: ['model.decoder.layers.12.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.12.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.encoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.13.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.14.encoder_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.14.encoder_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.14.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.13.encoder_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.15.encoder_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.13.encoder_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.encoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.encoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.out_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.13.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.13.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.15.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.14.encoder_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.embed_positions.weight', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n",
    "model = PegasusModel.from_pretrained(\"google/pegasus-x-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11920180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusTokenizerFast(name_or_path='google/pegasus-x-large', vocab_size=96103, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1afe9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studien haben gezeigt, dass das Besitzen eines Hundes gut für Sie ist.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# training\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"Chinese: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# studies have shown that owning a dog is good for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89746c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='t5-large', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a58719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁casserole': 25560,\n",
       " 'wear': 7258,\n",
       " 'Founder': 19145,\n",
       " '▁Rund': 10480,\n",
       " '▁Miami': 8327,\n",
       " '▁rebuilt': 26935,\n",
       " '▁conquer': 24025,\n",
       " '▁cazare': 19859,\n",
       " '▁accus': 17316,\n",
       " '▁job': 613,\n",
       " '▁Mercury': 23461,\n",
       " '▁Vil': 21340,\n",
       " '▁attempted': 13090,\n",
       " '▁meters': 8848,\n",
       " '▁br': 6397,\n",
       " 'premise': 17398,\n",
       " '▁alliance': 15454,\n",
       " 'kabel': 24606,\n",
       " '▁dép': 16257,\n",
       " '▁FUN': 26280,\n",
       " '▁introduced': 3665,\n",
       " '▁loose': 6044,\n",
       " '▁fade': 14201,\n",
       " '▁événements': 24243,\n",
       " '▁Forecast': 30175,\n",
       " '▁activități': 18244,\n",
       " 'TL': 12733,\n",
       " '▁vaccine': 12956,\n",
       " 'termin': 6544,\n",
       " '▁kilogram': 23332,\n",
       " '▁Up': 3234,\n",
       " 'berlin': 27995,\n",
       " '▁biscuit': 18922,\n",
       " 'illiard': 23827,\n",
       " 'Play': 15800,\n",
       " 'eilig': 26379,\n",
       " 'atter': 12828,\n",
       " '▁appraisal': 23555,\n",
       " 'pressed': 8918,\n",
       " '▁Clip': 17399,\n",
       " 'ktuell': 20046,\n",
       " 'versorgung': 24479,\n",
       " '▁Funny': 27364,\n",
       " '▁attraction': 13715,\n",
       " '▁quantitative': 18906,\n",
       " '▁planned': 4355,\n",
       " '▁Peugeot': 31288,\n",
       " '▁Haiti': 22179,\n",
       " '▁soon': 1116,\n",
       " '▁admin': 10473,\n",
       " 'oversized': 24698,\n",
       " '▁Stadium': 12750,\n",
       " 'schädigung': 30047,\n",
       " '▁conven': 14307,\n",
       " 'tribu': 5135,\n",
       " '99%': 21962,\n",
       " '▁inferior': 23447,\n",
       " 'hou': 9492,\n",
       " '▁1:1': 19520,\n",
       " 'EPA': 13986,\n",
       " '▁reporters': 19644,\n",
       " '▁ride': 2564,\n",
       " '▁unprecedented': 19534,\n",
       " '▁Slide': 21972,\n",
       " 'Mass': 27189,\n",
       " '▁perhaps': 2361,\n",
       " 'NI': 6197,\n",
       " '▁spin': 5404,\n",
       " '▁have': 43,\n",
       " '▁Seriously': 24159,\n",
       " '▁bad': 1282,\n",
       " '▁Intervention': 28499,\n",
       " '▁winners': 8969,\n",
       " 'parent': 12352,\n",
       " '▁avec': 393,\n",
       " 'bezüglich': 26915,\n",
       " 'oferă': 11285,\n",
       " '▁dull': 19511,\n",
       " '▁echipe': 20183,\n",
       " 'ionat': 8796,\n",
       " '▁rod': 6102,\n",
       " 'identifie': 8826,\n",
       " '▁Examples': 19119,\n",
       " '▁independence': 12315,\n",
       " '▁relentless': 29395,\n",
       " 'ătă': 17783,\n",
       " '▁acela': 11249,\n",
       " '▁printer': 6454,\n",
       " '▁cunosc': 22135,\n",
       " '▁Student': 6341,\n",
       " 'ivism': 30177,\n",
       " '▁Pyramid': 30237,\n",
       " '▁alive': 7267,\n",
       " '▁Kit': 5747,\n",
       " 'Central': 30497,\n",
       " '▁Jackie': 26868,\n",
       " 'ache': 4933,\n",
       " '▁transmit': 9153,\n",
       " 'urgence': 16786,\n",
       " '▁considér': 20228,\n",
       " '▁Regen': 12412,\n",
       " 'setzen': 5681,\n",
       " 'capsul': 30080,\n",
       " '▁calm': 4447,\n",
       " '▁Süd': 11242,\n",
       " 'deluxe': 27747,\n",
       " '▁attack': 3211,\n",
       " '▁licensed': 6681,\n",
       " 'destroying': 24512,\n",
       " '▁Emma': 15325,\n",
       " 'Blue': 22530,\n",
       " '▁Annual': 7389,\n",
       " '▁Flasche': 28639,\n",
       " 'efficiency': 31640,\n",
       " '▁minerals': 14288,\n",
       " 'UR': 5905,\n",
       " '▁Wert': 6954,\n",
       " '▁longest': 14783,\n",
       " '▁Joel': 22943,\n",
       " 'IAL': 15397,\n",
       " 'aves': 16001,\n",
       " '▁rapid': 3607,\n",
       " '▁happened': 2817,\n",
       " '▁warned': 15240,\n",
       " '▁individuals': 1742,\n",
       " '▁sarcina': 26326,\n",
       " '▁Vielfalt': 27579,\n",
       " '-18': 6996,\n",
       " 'built': 16152,\n",
       " '▁gern': 17390,\n",
       " 'nce': 3772,\n",
       " '▁begleiten': 27123,\n",
       " '▁Clickfunnel': 31399,\n",
       " '▁politically': 22937,\n",
       " '▁requested': 6709,\n",
       " '▁employee': 3490,\n",
       " '▁Verhalten': 20583,\n",
       " '▁participants': 3008,\n",
       " '▁Southern': 5193,\n",
       " 'bauen': 12323,\n",
       " '▁quasi': 16172,\n",
       " '▁twitter': 19010,\n",
       " '▁bugs': 13348,\n",
       " '▁Mr': 1363,\n",
       " '▁Whatever': 10406,\n",
       " 'BR': 6934,\n",
       " 'Akt': 19066,\n",
       " '▁Fun': 9259,\n",
       " '▁coupons': 11900,\n",
       " 'enseigne': 19157,\n",
       " '▁LGBT': 26398,\n",
       " '▁bucket': 11325,\n",
       " '▁Barbie': 27945,\n",
       " 'efectuare': 30769,\n",
       " '▁hilarious': 21566,\n",
       " 'TECH': 25036,\n",
       " '▁obtine': 17369,\n",
       " '▁Princeton': 26122,\n",
       " '▁FULL': 27645,\n",
       " '▁abroad': 7979,\n",
       " '▁frapp': 26050,\n",
       " '▁Schutz': 10956,\n",
       " '▁gracious': 24940,\n",
       " 'worn': 25237,\n",
       " '▁Tunnel': 27990,\n",
       " '▁Noah': 24384,\n",
       " 'utz': 9282,\n",
       " 'registering': 23455,\n",
       " '▁osteo': 23026,\n",
       " '▁Technik': 13204,\n",
       " '▁calendar': 4793,\n",
       " '▁Hier': 3204,\n",
       " '▁towel': 15580,\n",
       " 'philosoph': 17704,\n",
       " '▁limbaj': 28602,\n",
       " 'soare': 14266,\n",
       " '▁geometry': 23898,\n",
       " '▁réservation': 25329,\n",
       " '▁primordial': 30315,\n",
       " '▁specialize': 14721,\n",
       " '▁Stadi': 30408,\n",
       " '▁sont': 527,\n",
       " 'clam': 12818,\n",
       " 'Copiii': 30442,\n",
       " '▁diminished': 29055,\n",
       " '▁Leder': 23573,\n",
       " 'ène': 9720,\n",
       " '▁Geo': 10107,\n",
       " 'prinz': 23538,\n",
       " '▁nisip': 29389,\n",
       " '▁Gas': 6435,\n",
       " '▁Limited': 7363,\n",
       " '▁careers': 13325,\n",
       " '▁does': 405,\n",
       " '▁Congress': 4442,\n",
       " '▁Gau': 12520,\n",
       " 'VI': 7765,\n",
       " '▁situ': 12958,\n",
       " '▁arunc': 16656,\n",
       " 'tiv': 5499,\n",
       " '▁găsit': 18997,\n",
       " '▁protéger': 21289,\n",
       " 'RIT': 25066,\n",
       " '▁geprüft': 29322,\n",
       " 'ographer': 19891,\n",
       " '▁Legi': 17357,\n",
       " 'état': 7795,\n",
       " '▁fuss': 21904,\n",
       " '▁Sheriff': 22119,\n",
       " '▁Book': 3086,\n",
       " '▁Friedrich': 22785,\n",
       " 'acked': 13365,\n",
       " '▁voiture': 9820,\n",
       " '▁présenter': 17014,\n",
       " '▁Tournament': 20502,\n",
       " '▁prescri': 23911,\n",
       " 'awa': 7396,\n",
       " 'eklagte': 27457,\n",
       " '▁research': 585,\n",
       " '▁solving': 11795,\n",
       " '▁Sharon': 22482,\n",
       " '▁sleeping': 9182,\n",
       " '▁Important': 16516,\n",
       " '▁Carr': 11274,\n",
       " 'Just': 19969,\n",
       " '▁consortium': 27754,\n",
       " 'о': 2044,\n",
       " '▁inquiry': 15736,\n",
       " '▁dorinț': 29722,\n",
       " '▁Bezirks': 29357,\n",
       " 'pian': 21851,\n",
       " '▁foliage': 25840,\n",
       " '▁Constanţa': 30993,\n",
       " '▁nail': 11477,\n",
       " '▁court': 1614,\n",
       " 'ocupă': 26056,\n",
       " '▁Cardiff': 26911,\n",
       " '▁north': 3457,\n",
       " 'orul': 6486,\n",
       " 'sley': 8887,\n",
       " '▁janvier': 11186,\n",
       " '▁sewing': 15308,\n",
       " '▁Pinterest': 12476,\n",
       " '▁MAY': 27835,\n",
       " '▁console': 8990,\n",
       " '63': 3891,\n",
       " '▁scenes': 8073,\n",
       " '▁Example': 18792,\n",
       " 'amenajare': 30888,\n",
       " 'serez': 23422,\n",
       " '▁evidenț': 31699,\n",
       " 'bereich': 5967,\n",
       " '▁comprehensive': 3452,\n",
       " '▁propriété': 19713,\n",
       " '▁déposé': 28321,\n",
       " '▁Merci': 8923,\n",
       " '▁Supervisor': 25795,\n",
       " '▁physical': 1722,\n",
       " '▁février': 13728,\n",
       " '▁verbessern': 22372,\n",
       " '▁400': 4837,\n",
       " '▁adevărat': 11898,\n",
       " '▁premiere': 13539,\n",
       " 'aji': 17815,\n",
       " '▁oferi': 9061,\n",
       " '▁anticipated': 12723,\n",
       " '▁represents': 5475,\n",
       " '▁SERVICE': 26209,\n",
       " '▁emanat': 31975,\n",
       " '▁statute': 18692,\n",
       " '▁atac': 16339,\n",
       " '▁Blut': 15995,\n",
       " 'curated': 22579,\n",
       " 'alegerea': 21749,\n",
       " '▁pantalon': 27062,\n",
       " '▁Platform': 12779,\n",
       " 'utti': 31944,\n",
       " '▁Recherche': 22359,\n",
       " '▁insulation': 15298,\n",
       " '▁mock': 17812,\n",
       " '▁rosii': 28149,\n",
       " '▁transcription': 20267,\n",
       " '▁elbow': 23630,\n",
       " 'agh': 18583,\n",
       " '▁pension': 8645,\n",
       " '▁Legacy': 24843,\n",
       " '▁next': 416,\n",
       " '▁Namen': 10016,\n",
       " 'hov': 23304,\n",
       " '▁directory': 8174,\n",
       " '▁Groß': 8339,\n",
       " '▁goodness': 14838,\n",
       " '▁Landkreis': 27895,\n",
       " 'prevailing': 29370,\n",
       " 'part': 2274,\n",
       " '▁Consul': 11540,\n",
       " '▁bracelet': 15073,\n",
       " '▁Schedule': 14890,\n",
       " 'dincolo': 26242,\n",
       " 'seized': 27217,\n",
       " 'punerea': 21651,\n",
       " 'should': 25351,\n",
       " '▁wishes': 9543,\n",
       " '▁faucet': 17471,\n",
       " '▁Landes': 9043,\n",
       " '▁Lim': 10908,\n",
       " 'QU': 16892,\n",
       " '▁expenses': 5159,\n",
       " '▁farmers': 7208,\n",
       " '▁renting': 17992,\n",
       " 'Live': 24179,\n",
       " 'adia': 17327,\n",
       " '▁Pascal': 25402,\n",
       " 'stattet': 28707,\n",
       " '▁idei': 16176,\n",
       " '▁evrei': 30458,\n",
       " '▁1900': 19036,\n",
       " 'urgeon': 19623,\n",
       " '▁même': 1398,\n",
       " '▁complete': 743,\n",
       " 'existence': 22597,\n",
       " '▁let': 752,\n",
       " '▁adventurous': 23985,\n",
       " '▁Republicans': 15623,\n",
       " '▁Tatsache': 27712,\n",
       " '▁scenarios': 13911,\n",
       " 'illage': 17614,\n",
       " '▁assignments': 14023,\n",
       " '▁cocktails': 19005,\n",
       " '▁spoke': 5468,\n",
       " '▁chase': 15389,\n",
       " '▁müsst': 20054,\n",
       " '103': 17864,\n",
       " '▁pumpkin': 13790,\n",
       " '▁dome': 22161,\n",
       " '▁Enforcement': 27075,\n",
       " '▁pot': 815,\n",
       " '▁erfolgt': 10168,\n",
       " '▁Focus': 11561,\n",
       " '▁Ghi': 20730,\n",
       " '▁Landscape': 20834,\n",
       " '▁chorus': 26681,\n",
       " '▁façade': 24369,\n",
       " '▁homeowner': 18559,\n",
       " '▁près': 9800,\n",
       " '▁estimate': 7037,\n",
       " 'enregistrement': 28333,\n",
       " '▁toothpaste': 31916,\n",
       " '▁liking': 23340,\n",
       " 'VR': 13556,\n",
       " '▁Ghana': 18406,\n",
       " '▁Alternative': 13661,\n",
       " '▁Loop': 23456,\n",
       " '▁brand': 1056,\n",
       " '▁portion': 4149,\n",
       " '▁Verwendung': 17928,\n",
       " '▁Fahrzeug': 11786,\n",
       " '▁candy': 11617,\n",
       " '▁privilégi': 23100,\n",
       " 'erfolgreichen': 30725,\n",
       " '▁Gö': 19563,\n",
       " '▁BA': 8145,\n",
       " '▁attributes': 12978,\n",
       " '▁revue': 26113,\n",
       " '▁simultan': 30445,\n",
       " '▁falling': 7306,\n",
       " 'uck': 4636,\n",
       " '▁cauta': 18544,\n",
       " '▁Raven': 20744,\n",
       " 'ambiance': 15145,\n",
       " '▁founder': 7174,\n",
       " 'éloign': 29041,\n",
       " 'elemente': 15619,\n",
       " '▁literally': 6672,\n",
       " '▁Earth': 4030,\n",
       " 'break': 14577,\n",
       " '▁député': 28096,\n",
       " 'vêtement': 28885,\n",
       " '▁avail': 17671,\n",
       " '▁să': 246,\n",
       " '▁Hook': 21896,\n",
       " '▁For': 242,\n",
       " '▁Venue': 29940,\n",
       " 'rib': 6520,\n",
       " '▁calculated': 11338,\n",
       " '▁unique': 775,\n",
       " '▁acolo': 6673,\n",
       " '▁reckless': 28933,\n",
       " 'exemplar': 26710,\n",
       " 'mirrored': 31709,\n",
       " '0.7': 22426,\n",
       " '▁entscheiden': 10767,\n",
       " 'ation': 257,\n",
       " 'sweet': 23392,\n",
       " '▁Termin': 10181,\n",
       " '▁lounge': 9658,\n",
       " '▁Heights': 17948,\n",
       " '▁environments': 8258,\n",
       " '▁Jusqu': 31398,\n",
       " '▁tehnici': 23234,\n",
       " '▁caz': 6753,\n",
       " '▁decide': 2204,\n",
       " '▁unlock': 12502,\n",
       " '▁permet': 2933,\n",
       " '▁brushes': 23159,\n",
       " '▁dat': 3927,\n",
       " '▁Help': 5090,\n",
       " 'click': 12570,\n",
       " '▁song': 2324,\n",
       " '▁HBO': 27534,\n",
       " '▁Bali': 20241,\n",
       " '▁bani': 7274,\n",
       " 'dev': 9776,\n",
       " '▁werde': 10808,\n",
       " 'gut': 11221,\n",
       " '▁steam': 7222,\n",
       " '▁gamme': 10994,\n",
       " '▁subscription': 7644,\n",
       " '▁Metropolitan': 20074,\n",
       " '▁measure': 3613,\n",
       " 'athlet': 26170,\n",
       " '▁Dow': 21236,\n",
       " 'elia': 13240,\n",
       " 'FF': 9089,\n",
       " 'Tag': 23593,\n",
       " '▁knife': 10821,\n",
       " '▁speaks': 12192,\n",
       " '▁asked': 1380,\n",
       " '▁charging': 10871,\n",
       " '▁Norway': 16491,\n",
       " '▁lumber': 26467,\n",
       " 'Chef': 29639,\n",
       " '▁pâr': 31938,\n",
       " '▁luxury': 3873,\n",
       " '▁Rising': 25813,\n",
       " '▁Folgen': 20019,\n",
       " '▁grandfather': 18573,\n",
       " '▁Ferguson': 27461,\n",
       " '▁Residential': 15773,\n",
       " '▁length': 2475,\n",
       " '▁DAMAGE': 30061,\n",
       " '264': 26755,\n",
       " '▁garant': 16760,\n",
       " '▁Hwy': 29171,\n",
       " '▁treatment': 1058,\n",
       " '▁confidentiality': 23979,\n",
       " 'folgen': 13770,\n",
       " '23': 2773,\n",
       " '03': 4928,\n",
       " '▁hiding': 17810,\n",
       " 'chin': 5675,\n",
       " '▁career': 1415,\n",
       " '▁scenery': 15806,\n",
       " '▁priest': 12966,\n",
       " '▁cazul': 3824,\n",
       " 'rul': 5155,\n",
       " '▁deadline': 7183,\n",
       " '▁compromis': 29296,\n",
       " 'oral': 8563,\n",
       " 'WA': 12054,\n",
       " 'gold': 14910,\n",
       " 'KA': 12048,\n",
       " 'DES': 18284,\n",
       " 'fel': 4025,\n",
       " '▁Dashboard': 30424,\n",
       " '▁Shi': 4804,\n",
       " '▁firms': 5654,\n",
       " '▁Neben': 7974,\n",
       " 'Save': 23163,\n",
       " 'ред': 30051,\n",
       " '▁Durham': 25215,\n",
       " 'Pourtant': 27186,\n",
       " '▁Alexandria': 25664,\n",
       " '▁Social': 2730,\n",
       " '▁ajutorul': 10082,\n",
       " 'quarter': 19973,\n",
       " '▁Warriors': 24499,\n",
       " 'caster': 25327,\n",
       " 'icrobial': 27543,\n",
       " '▁marina': 16584,\n",
       " '▁Plenty': 28474,\n",
       " '▁soutenu': 30362,\n",
       " '▁righteousness': 29950,\n",
       " 'Hence': 13151,\n",
       " '▁Übersetzung': 30529,\n",
       " '▁formal': 4727,\n",
       " '▁chinez': 29240,\n",
       " 'Können': 30862,\n",
       " '▁coaches': 11210,\n",
       " '▁Dorothy': 29039,\n",
       " '▁tête': 9844,\n",
       " '▁aparțin': 31508,\n",
       " 'gar': 1478,\n",
       " '▁Patient': 17656,\n",
       " '▁intrigued': 27906,\n",
       " '▁Before': 3103,\n",
       " '▁cantitate': 26161,\n",
       " '▁productivity': 7596,\n",
       " '▁oils': 10229,\n",
       " '▁Active': 11383,\n",
       " '▁compact': 6607,\n",
       " '▁transmitter': 27707,\n",
       " '▁taxpayer': 15375,\n",
       " '▁rasp': 22501,\n",
       " '▁smile': 3993,\n",
       " 'verkehr': 17105,\n",
       " '▁actions': 2874,\n",
       " '▁centuries': 11653,\n",
       " '▁Sigur': 28432,\n",
       " '▁review': 1132,\n",
       " 'Etat': 15890,\n",
       " 'gathered': 9094,\n",
       " '▁hole': 6356,\n",
       " '▁assembled': 17583,\n",
       " '▁tactics': 15541,\n",
       " '▁gains': 11391,\n",
       " '▁Hub': 10261,\n",
       " '▁hran': 21971,\n",
       " '▁watched': 7533,\n",
       " 'Archived': 25771,\n",
       " '▁blanc': 8753,\n",
       " 'System': 14342,\n",
       " '▁precedent': 17875,\n",
       " 'typical': 21888,\n",
       " 'preşedintele': 24098,\n",
       " '▁suspended': 14840,\n",
       " 'CENT': 22851,\n",
       " '▁vergleichen': 25132,\n",
       " '▁chinois': 21717,\n",
       " '▁Respect': 26795,\n",
       " 'friendly': 4905,\n",
       " '▁snuggl': 30800,\n",
       " 'shot': 11159,\n",
       " 'Thierry': 30987,\n",
       " '▁camping': 8515,\n",
       " '▁perdre': 23013,\n",
       " 'ții': 3544,\n",
       " '▁Standing': 23715,\n",
       " '▁bilateral': 24097,\n",
       " '▁geschie': 30686,\n",
       " '▁IEEE': 23570,\n",
       " '▁Cit': 11224,\n",
       " '▁comply': 9095,\n",
       " '▁mal': 1460,\n",
       " '▁Strong': 16366,\n",
       " '▁correct': 2024,\n",
       " '▁starts': 3511,\n",
       " '▁reg': 5925,\n",
       " '▁Determine': 30197,\n",
       " 'ction': 4985,\n",
       " 'owohl': 11413,\n",
       " '▁servi': 13482,\n",
       " 'flexion': 31898,\n",
       " '▁Vorteile': 21731,\n",
       " '?\"': 4609,\n",
       " '▁sets': 3369,\n",
       " '▁pielea': 17329,\n",
       " '▁Accept': 20592,\n",
       " '▁Converse': 27668,\n",
       " '▁moisture': 9576,\n",
       " '▁ihm': 6889,\n",
       " '▁compensate': 17077,\n",
       " '▁Currency': 31383,\n",
       " '▁flesh': 15634,\n",
       " 'tab': 10309,\n",
       " '▁Ja': 2215,\n",
       " 'environnement': 8777,\n",
       " '▁commemorat': 18681,\n",
       " '▁fireworks': 23806,\n",
       " 'scheid': 10726,\n",
       " 'Pentru': 30953,\n",
       " 'ami': 3690,\n",
       " '▁occasion': 5333,\n",
       " '▁sailors': 30899,\n",
       " '▁shrub': 21675,\n",
       " '▁plagiarism': 31447,\n",
       " '▁pumps': 16646,\n",
       " '▁features': 753,\n",
       " 'chon': 17128,\n",
       " '▁guarantee': 3614,\n",
       " '▁displayed': 6099,\n",
       " '▁follows': 6963,\n",
       " '▁Troy': 21854,\n",
       " '▁Retrieve': 8298,\n",
       " '▁vot': 8164,\n",
       " 'lase': 25718,\n",
       " '▁overload': 26343,\n",
       " 'verschiedenen': 7811,\n",
       " 'weilige': 29904,\n",
       " '▁weapons': 7749,\n",
       " '▁shallow': 16906,\n",
       " '▁android': 17729,\n",
       " '▁TRANS': 26585,\n",
       " 'impl': 10296,\n",
       " '▁shot': 2538,\n",
       " '▁genau': 5524,\n",
       " '▁MacBook': 27084,\n",
       " '▁gezeigt': 22985,\n",
       " 'aku': 16296,\n",
       " '▁Geld': 6213,\n",
       " 'xton': 20706,\n",
       " 'mment': 15699,\n",
       " 'posi': 19882,\n",
       " '▁chefs': 16863,\n",
       " 'ist': 343,\n",
       " '▁SSD': 20947,\n",
       " '▁vulnerability': 21279,\n",
       " 'grupului': 30612,\n",
       " '▁laughter': 22496,\n",
       " '▁erwartet': 17802,\n",
       " 'bag': 7893,\n",
       " '▁prison': 5714,\n",
       " '▁ANY': 10568,\n",
       " '▁Kunst': 9219,\n",
       " '▁freezing': 19275,\n",
       " 'finish': 25535,\n",
       " '▁Nouveau': 28016,\n",
       " '▁Though': 4229,\n",
       " '▁requirements': 1502,\n",
       " '▁medieval': 16493,\n",
       " '▁Blade': 23772,\n",
       " '▁verfüge': 23820,\n",
       " '▁reliabl': 28624,\n",
       " '▁Într': 24503,\n",
       " 'what': 9170,\n",
       " '▁Antrag': 20647,\n",
       " '▁Leitung': 22867,\n",
       " 'Compared': 25236,\n",
       " '▁Vent': 13713,\n",
       " 'kunst': 23928,\n",
       " 'teur': 5348,\n",
       " '▁shake': 8944,\n",
       " 'am': 265,\n",
       " '▁vacances': 13941,\n",
       " 'nect': 20902,\n",
       " '▁Equ': 25262,\n",
       " '▁pop': 2783,\n",
       " '▁dam': 10157,\n",
       " '▁Actual': 24193,\n",
       " '▁Neck': 25800,\n",
       " '▁Fußball': 20277,\n",
       " '▁death': 1687,\n",
       " 'vid': 6961,\n",
       " '▁via': 1009,\n",
       " '▁daca': 2183,\n",
       " '▁tidy': 22810,\n",
       " 'OW': 15251,\n",
       " 'ären': 15540,\n",
       " '▁And': 275,\n",
       " '▁benign': 31144,\n",
       " '▁groupe': 4986,\n",
       " 'oase': 4722,\n",
       " '▁mondial': 12175,\n",
       " '▁accompany': 12235,\n",
       " '▁diplôme': 29831,\n",
       " '▁Other': 2502,\n",
       " 'triggering': 31783,\n",
       " '▁bulk': 7942,\n",
       " '▁faţă': 13028,\n",
       " 'ggy': 9559,\n",
       " '▁Low': 5586,\n",
       " 'ised': 3375,\n",
       " 'Inde': 26267,\n",
       " '▁percentage': 5294,\n",
       " 'UMP': 28468,\n",
       " '▁know': 214,\n",
       " '▁harm': 6263,\n",
       " 'PAL': 28309,\n",
       " '▁Champ': 22465,\n",
       " 'udder': 28002,\n",
       " 'inauguration': 30634,\n",
       " 'MM': 8257,\n",
       " '▁Arbor': 27560,\n",
       " '▁strengthen': 8726,\n",
       " '45': 2128,\n",
       " '▁Facilit': 29986,\n",
       " '▁alt': 4445,\n",
       " 'Firstly': 23559,\n",
       " 'pet': 4995,\n",
       " 'vert': 3027,\n",
       " '▁advantage': 2337,\n",
       " '▁încercat': 25807,\n",
       " '▁fully': 1540,\n",
       " '▁hold': 1520,\n",
       " '▁Mädchen': 21644,\n",
       " '▁Impact': 14906,\n",
       " '▁persuasive': 29535,\n",
       " '▁Lay': 17726,\n",
       " '▁sing': 10159,\n",
       " '▁Tell': 8779,\n",
       " '▁rolling': 8394,\n",
       " '▁biblical': 23438,\n",
       " 'para': 6583,\n",
       " '▁Terri': 17495,\n",
       " '▁va': 409,\n",
       " 'omics': 27036,\n",
       " '▁bout': 9338,\n",
       " '▁compassion': 14555,\n",
       " '▁Limo': 21866,\n",
       " 'AIS': 25018,\n",
       " '▁bleiben': 10268,\n",
       " '▁Commercial': 9747,\n",
       " '▁surse': 20130,\n",
       " '▁piept': 28294,\n",
       " '▁municipalities': 24858,\n",
       " 'ndererseits': 29655,\n",
       " '▁dependence': 24264,\n",
       " 'RAF': 27380,\n",
       " '▁vers': 2676,\n",
       " '▁schedule': 2023,\n",
       " '▁doar': 1617,\n",
       " 'oxi': 14443,\n",
       " 'OUR': 9131,\n",
       " '▁upward': 14452,\n",
       " 'rühren': 26969,\n",
       " '▁acestor': 10359,\n",
       " '▁cohort': 23785,\n",
       " 'CK': 10459,\n",
       " '▁serie': 12480,\n",
       " 'jungen': 21000,\n",
       " '0.4': 22776,\n",
       " '▁Stamm': 25009,\n",
       " '▁username': 20304,\n",
       " '▁1988': 10414,\n",
       " '▁Viagra': 24061,\n",
       " '▁feasible': 20218,\n",
       " 'aventure': 17143,\n",
       " '▁Nürnberg': 28211,\n",
       " '▁temporar': 28650,\n",
       " 'SSA': 26787,\n",
       " '▁Gericht': 16419,\n",
       " '▁Jamie': 17845,\n",
       " '▁smaller': 2755,\n",
       " '▁délivr': 28861,\n",
       " 'któ': 30479,\n",
       " 'ettes': 16167,\n",
       " 'performing': 27245,\n",
       " '▁platforms': 5357,\n",
       " '▁Regulation': 13683,\n",
       " 'tex': 10354,\n",
       " 'ivi': 11687,\n",
       " '▁pencil': 13966,\n",
       " '▁commenc': 30866,\n",
       " 'ennemi': 31025,\n",
       " 'effi': 23473,\n",
       " '▁debris': 12544,\n",
       " '▁ambient': 21128,\n",
       " '▁Rom': 12583,\n",
       " '▁scris': 8294,\n",
       " '▁Gre': 7186,\n",
       " 'Quelques': 29134,\n",
       " 'ersten': 3593,\n",
       " '▁damn': 17227,\n",
       " 'keeping': 14361,\n",
       " '▁profiles': 10958,\n",
       " '▁chimic': 23857,\n",
       " 'formular': 20128,\n",
       " '▁LG': 13444,\n",
       " '▁atunci': 2732,\n",
       " 'odia': 23193,\n",
       " '▁contrast': 4656,\n",
       " 'tap': 8873,\n",
       " '▁financially': 16851,\n",
       " '▁secol': 17515,\n",
       " 'izi': 10954,\n",
       " '▁Zar': 24374,\n",
       " '▁His': 978,\n",
       " 'util': 13780,\n",
       " '▁times': 648,\n",
       " 'courtesy': 14726,\n",
       " '▁wonderful': 1627,\n",
       " '▁Rich': 10825,\n",
       " 'lbs': 9949,\n",
       " '▁Oster': 22054,\n",
       " '▁Lounge': 18854,\n",
       " '▁Bild': 6689,\n",
       " '▁Indo': 26822,\n",
       " '▁integration': 5660,\n",
       " 'mat': 3357,\n",
       " '▁enroll': 17990,\n",
       " '▁Salt': 11888,\n",
       " '▁retrieve': 21830,\n",
       " '▁offices': 6036,\n",
       " '▁Six': 7643,\n",
       " '▁folos': 16206,\n",
       " '▁livr': 16965,\n",
       " 'Message': 16042,\n",
       " '233': 20879,\n",
       " '▁meat': 3604,\n",
       " '▁commonly': 5871,\n",
       " 'flow': 7631,\n",
       " '▁Factory': 15372,\n",
       " 'volution': 24817,\n",
       " 'fact': 8717,\n",
       " 'presenting': 12072,\n",
       " '▁Fantasy': 19202,\n",
       " 'idos': 28594,\n",
       " '▁Muslims': 16932,\n",
       " 'affirmed': 30511,\n",
       " 'cited': 11675,\n",
       " '▁kostet': 22907,\n",
       " '▁spati': 19547,\n",
       " 'leger': 23196,\n",
       " '▁chapter': 5800,\n",
       " '▁propaganda': 25071,\n",
       " '▁sollen': 9225,\n",
       " 'ugg': 13917,\n",
       " '▁Comfort': 15930,\n",
       " 'klima': 29348,\n",
       " '▁Wahrscheinlich': 31661,\n",
       " '▁Anforderungen': 17707,\n",
       " 'erweise': 11164,\n",
       " 'owed': 9200,\n",
       " '▁sensitive': 6280,\n",
       " '▁dealer': 10543,\n",
       " '▁poly': 4251,\n",
       " '▁înseamn': 12506,\n",
       " 'quelles': 17118,\n",
       " '▁sign': 1320,\n",
       " '▁wrap': 6215,\n",
       " 'party': 8071,\n",
       " '▁mild': 8248,\n",
       " '▁fire': 1472,\n",
       " '▁verify': 10446,\n",
       " '▁Rou': 10898,\n",
       " '▁Cedar': 21838,\n",
       " '▁versatility': 23761,\n",
       " 'communication': 28921,\n",
       " '▁Krankenhaus': 29869,\n",
       " '▁Officer': 6027,\n",
       " '▁objectives': 7233,\n",
       " '▁prize': 6441,\n",
       " '▁obligations': 9958,\n",
       " 'converting': 21049,\n",
       " 'pra': 5319,\n",
       " '▁recording': 5592,\n",
       " '▁crore': 23095,\n",
       " '▁informal': 15347,\n",
       " '▁Cent': 18434,\n",
       " '▁Willkommen': 29356,\n",
       " '▁cherch': 8909,\n",
       " '▁junior': 9212,\n",
       " 'längst': 24683,\n",
       " '▁simulate': 26032,\n",
       " '▁Cream': 14024,\n",
       " '▁soutien': 15203,\n",
       " '▁communal': 22393,\n",
       " '▁Wand': 13185,\n",
       " '▁15%': 13914,\n",
       " 'different': 25880,\n",
       " 'Gar': 21846,\n",
       " '▁Angriff': 25717,\n",
       " '_': 834,\n",
       " 'abord': 5587,\n",
       " '▁alte': 2105,\n",
       " '▁vice': 6444,\n",
       " '▁luna': 8476,\n",
       " '▁Publikum': 22692,\n",
       " 'dents': 24180,\n",
       " 'gression': 22430,\n",
       " '▁World': 1150,\n",
       " 'cred': 17216,\n",
       " 'rium': 11879,\n",
       " 'GET': 20750,\n",
       " '▁Built': 14862,\n",
       " '/10': 11476,\n",
       " '▁About': 4504,\n",
       " '▁pdf': 9210,\n",
       " '▁$100': 10417,\n",
       " '▁scissors': 28958,\n",
       " '▁Quel': 12281,\n",
       " '▁Dans': 3039,\n",
       " '▁itinerary': 23690,\n",
       " 'reisen': 19800,\n",
       " '▁Such': 3900,\n",
       " '▁passionné': 25846,\n",
       " '▁Than': 19224,\n",
       " 'fordern': 21390,\n",
       " 'isé': 6956,\n",
       " '▁Pdf': 28630,\n",
       " '▁angajați': 29054,\n",
       " '▁disappear': 10587,\n",
       " '▁grace': 8140,\n",
       " '▁Wege': 18586,\n",
       " '▁downtown': 7092,\n",
       " '▁poetic': 25614,\n",
       " '▁manipulate': 23307,\n",
       " '▁bathrooms': 14704,\n",
       " 'gegangen': 16416,\n",
       " '▁Wir': 1185,\n",
       " 'Back': 21106,\n",
       " '▁patron': 9331,\n",
       " 'Writ': 24965,\n",
       " '▁Japan': 3411,\n",
       " '6%': 6370,\n",
       " '▁erstellen': 21239,\n",
       " 'ä': 1864,\n",
       " 'AMA': 21250,\n",
       " '▁spectacular': 8254,\n",
       " '▁spiral': 17723,\n",
       " '▁Guvernul': 25368,\n",
       " '▁their': 70,\n",
       " 'ram': 2375,\n",
       " '▁quantity': 8708,\n",
       " 'fine': 13536,\n",
       " 'Mbps': 27715,\n",
       " 'lash': 8058,\n",
       " '▁Serving': 24964,\n",
       " 'hill': 6321,\n",
       " '▁bike': 3724,\n",
       " '▁faim': 27083,\n",
       " '▁Milton': 26805,\n",
       " '▁wi': 11064,\n",
       " '▁rebellion': 31430,\n",
       " '▁Portugal': 12627,\n",
       " '▁donne': 8551,\n",
       " '▁erase': 22015,\n",
       " 'even': 6190,\n",
       " 'design': 9124,\n",
       " 'occurrence': 16526,\n",
       " '▁Monday': 2089,\n",
       " 'MO': 5365,\n",
       " '▁tightly': 20994,\n",
       " '▁160': 11321,\n",
       " 'phi': 11692,\n",
       " '▁spontan': 22234,\n",
       " '▁sowie': 1885,\n",
       " 'rim': 5397,\n",
       " '▁Eli': 7495,\n",
       " '▁fame': 10393,\n",
       " '▁usor': 8189,\n",
       " '▁tannin': 25117,\n",
       " '▁Une': 2435,\n",
       " 'setzt': 6773,\n",
       " '▁ochi': 12356,\n",
       " '▁computer': 1218,\n",
       " '▁Zwei': 11280,\n",
       " '160': 19129,\n",
       " 'gebucht': 29974,\n",
       " '▁apoi': 4809,\n",
       " '▁giveaway': 14893,\n",
       " '▁believe': 857,\n",
       " '▁editing': 8414,\n",
       " '▁1961': 21018,\n",
       " '▁argint': 26090,\n",
       " '▁omega': 27305,\n",
       " '▁severely': 20215,\n",
       " 'sozialen': 24105,\n",
       " '▁umbrella': 18598,\n",
       " '▁participer': 19283,\n",
       " '▁isolated': 12996,\n",
       " '▁darauf': 6101,\n",
       " '▁tourists': 11618,\n",
       " '▁Manage': 19607,\n",
       " 'Bio': 26475,\n",
       " '▁media': 783,\n",
       " 'specialized': 8689,\n",
       " '▁Long': 3230,\n",
       " '▁explanation': 7295,\n",
       " '▁punish': 24584,\n",
       " '▁immigration': 10653,\n",
       " 'male': 13513,\n",
       " '▁linii': 26620,\n",
       " '▁manufacturing': 3732,\n",
       " '▁guest': 3886,\n",
       " 'verarbeitung': 28903,\n",
       " '2.1': 14489,\n",
       " '▁territories': 23995,\n",
       " '▁richesse': 28093,\n",
       " 'Direct': 23620,\n",
       " '▁Acces': 25679,\n",
       " 'phen': 19017,\n",
       " '▁according': 1315,\n",
       " '▁Cognitive': 31109,\n",
       " '▁customer': 884,\n",
       " '▁earth': 3596,\n",
       " '▁Ra': 2922,\n",
       " '▁decent': 7162,\n",
       " '▁elevator': 19967,\n",
       " 'qué': 7195,\n",
       " '▁unele': 9482,\n",
       " 'inevitably': 21874,\n",
       " 'bottom': 30142,\n",
       " '▁Functional': 27155,\n",
       " '▁văzut': 14516,\n",
       " 'un': 202,\n",
       " 'luft': 20620,\n",
       " 'schätze': 23388,\n",
       " '▁affinity': 30352,\n",
       " '▁ranch': 17008,\n",
       " '▁Understand': 25647,\n",
       " '▁Trent': 24116,\n",
       " 'hort': 14184,\n",
       " '▁(1)': 5637,\n",
       " '177': 26793,\n",
       " '▁Additional': 11180,\n",
       " '▁tragic': 17414,\n",
       " '▁Basket': 26029,\n",
       " '▁(7': 13649,\n",
       " '▁western': 8282,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93a25b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52508d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684f770c3e714a6784811c15148e0484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4825df7a8d9748b493303b7429a800ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6029a512234cfba0ccb8e536c7eb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/rh/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3707: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1215951/2394063116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"UN Chief Says There Is No Military Solution in Syria\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_seq2seq_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en_XX\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtranslated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang_code_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ro_RO\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Şeful ONU declară că nu există o soluţie militară în Siria\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/rh/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/rh/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         )\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# 4. Define other model kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\")\n",
    "article = \"UN Chief Says There Is No Military Solution in Syria\"\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], src_lang=\"en_XX\")\n",
    "translated_tokens = model.generate(**batch, decoder_start_token_id=tokenizer.lang_code_to_id[\"ro_RO\"])\n",
    "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "assert translation == \"Şeful ONU declară că nu există o soluţie militară în Siria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad694f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartTokenizer(name_or_path='facebook/mbart-large-en-ro', vocab_size=250027, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['ar_AR', 'cs_CZ', 'de_DE', 'en_XX', 'es_XX', 'et_EE', 'fi_FI', 'fr_XX', 'gu_IN', 'hi_IN', 'it_IT', 'ja_XX', 'kk_KZ', 'ko_KR', 'lt_LT', 'lv_LV', 'my_MM', 'ne_NP', 'nl_XX', 'ro_RO', 'ru_RU', 'si_LK', 'tr_TR', 'vi_VN', 'zh_CN']})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d72cfdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11dcf3a6340464ca311d6212742332a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b025cbff78fc4c31a8dcb79b3d8f6aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f50e1789cf4f30bbb73f293294d421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60083a4d2abc492c8f101e93c5726df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AlbertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8842f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertTokenizerFast(name_or_path='albert-base-v2', vocab_size=30000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<pad>', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5725e912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'great', 'all', 'really', 'very']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(input_ids).logits\n",
    "\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "\n",
    "tokenizer.decode(predictions).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c7c0830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizerFast(name_or_path='facebook/bart-large', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7c84e107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "74e0f3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Storage': 46656,\n",
       " 'ict': 11726,\n",
       " 'ĠSoviets': 40971,\n",
       " 'iet': 5810,\n",
       " 'Ġsang': 11944,\n",
       " 'ĠWilliams': 1604,\n",
       " 'Ġlinger': 18277,\n",
       " 'Ġcereal': 25629,\n",
       " 'ĠPaul': 1206,\n",
       " 'ĠFBI': 2448,\n",
       " 'Clinton': 36206,\n",
       " 'Season': 37052,\n",
       " 'ĠOlivia': 11924,\n",
       " 'Ġgelatin': 44496,\n",
       " 'Ġforests': 14275,\n",
       " 'rington': 12344,\n",
       " 'Ġaerospace': 15064,\n",
       " 'ĠMartha': 15281,\n",
       " 'osed': 7878,\n",
       " 'Ġaver': 25542,\n",
       " 'Ġmimic': 26361,\n",
       " 'ĠâĢķ': 15130,\n",
       " 'ĠPortsmouth': 19611,\n",
       " 'ĠBUT': 30518,\n",
       " 'Ġpretended': 35793,\n",
       " 'ĠResort': 10989,\n",
       " 'Jam': 39503,\n",
       " 'ĠNASL': 47179,\n",
       " 'Ġwardrobe': 18704,\n",
       " 'Ġmodify': 23209,\n",
       " 'Ġlure': 16679,\n",
       " 'Do': 8275,\n",
       " 'Ġchose': 4689,\n",
       " 'Ġbolstered': 22689,\n",
       " 'Ġcu': 3729,\n",
       " 'ĠAltern': 33958,\n",
       " 'Ġuncomfortable': 9800,\n",
       " 'bank': 5760,\n",
       " 'Ġ+++': 41500,\n",
       " 'Ġfile': 2870,\n",
       " 'ĠMyth': 36523,\n",
       " 'il': 718,\n",
       " 'Seattle': 40718,\n",
       " 'idency': 41059,\n",
       " 'Ġdiplom': 31038,\n",
       " 'Back': 19085,\n",
       " 'Drop': 43542,\n",
       " 'hea': 26343,\n",
       " 'ĠLaur': 25722,\n",
       " 'owder': 43167,\n",
       " 'iazep': 44589,\n",
       " 'ĠNext': 4130,\n",
       " 'ĠNotice': 22873,\n",
       " 'Often': 39972,\n",
       " 'aucas': 47317,\n",
       " 'Ranked': 48807,\n",
       " 'loo': 37521,\n",
       " 'ĠStew': 28154,\n",
       " 'ĠITS': 27409,\n",
       " 'La': 10766,\n",
       " 'ĠBlack': 1378,\n",
       " 'Ġion': 31973,\n",
       " 'Ġassistant': 3167,\n",
       " 'Ġloyalty': 10177,\n",
       " 'Ġbits': 15239,\n",
       " 'ĠTP': 31847,\n",
       " 'sec': 8584,\n",
       " 'Ġcaught': 2037,\n",
       " 'Ġclam': 24045,\n",
       " 'forming': 11847,\n",
       " 'ĠPunjab': 8177,\n",
       " 'ĠSecretary': 1863,\n",
       " 'ĠAfter': 572,\n",
       " 'ĠFrancis': 5075,\n",
       " 'ĠParkway': 15787,\n",
       " 'cycl': 19297,\n",
       " 'ĠActivity': 26313,\n",
       " 'Ġrounding': 31144,\n",
       " 'Ġumb': 31527,\n",
       " '224': 28835,\n",
       " 'ĠDiablo': 37238,\n",
       " 'Ġcondom': 37320,\n",
       " 'Ġpicture': 2170,\n",
       " 'Ġdiscard': 37952,\n",
       " 'ĠStard': 42106,\n",
       " 'ĠMSM': 43596,\n",
       " '\"âĢĶ': 24337,\n",
       " 'ĠWiki': 45569,\n",
       " 'Ġpid': 46616,\n",
       " 'ĠChrist': 4845,\n",
       " 'Ġcoordinated': 13662,\n",
       " 'Saharan': 27692,\n",
       " 'urus': 37716,\n",
       " 'ĠBrooke': 17806,\n",
       " 'Ġbrethren': 41673,\n",
       " 'Ġbelonged': 14574,\n",
       " 'ĠCyprus': 13303,\n",
       " 'Ġentertaining': 11110,\n",
       " 'Ġpatrols': 20315,\n",
       " 'Ġsketches': 31397,\n",
       " 'ĠDepth': 43553,\n",
       " 'demand': 15509,\n",
       " 'Ġpasta': 18236,\n",
       " 'Ġsightings': 26619,\n",
       " 'Ġ344': 40761,\n",
       " 'Ġversions': 7952,\n",
       " 'Chall': 41584,\n",
       " 'Ġtopping': 11744,\n",
       " 'IPP': 33950,\n",
       " 'Ġnihil': 45063,\n",
       " 'Ġcampaigners': 21952,\n",
       " 'bernatorial': 46639,\n",
       " 'Frame': 46766,\n",
       " 'ĠWARN': 47612,\n",
       " 'good': 8396,\n",
       " 'Container': 48557,\n",
       " 'Ġcountries': 749,\n",
       " 'Ġmethod': 5448,\n",
       " 'ĠGhana': 5498,\n",
       " 'Ġfeatures': 1575,\n",
       " 'err': 14385,\n",
       " 'DIS': 37056,\n",
       " 'ĠOff': 4995,\n",
       " 'ĠBesides': 8705,\n",
       " 'Ġenergies': 34185,\n",
       " 'Ġcredited': 11223,\n",
       " 'Ġenrolled': 12751,\n",
       " '409': 34295,\n",
       " 'Ġsuspension': 5436,\n",
       " '384': 35324,\n",
       " 'ĠADS': 37450,\n",
       " 'Ġseems': 1302,\n",
       " 'Ġunpop': 48489,\n",
       " '.<': 49069,\n",
       " 'Ġmainland': 11280,\n",
       " 'Ġundocumented': 13573,\n",
       " 'Ġcompositions': 34149,\n",
       " 'Ġnewest': 8946,\n",
       " 'Ġmeaningless': 27782,\n",
       " 'Autom': 39184,\n",
       " 'ECH': 32958,\n",
       " 'ĠLands': 24380,\n",
       " 'treatment': 37558,\n",
       " 'Ġmagnetic': 17512,\n",
       " 'Ġdeepest': 19762,\n",
       " 'Ġnightmares': 31634,\n",
       " 'Ġinfrared': 30175,\n",
       " 'ĠPredator': 36813,\n",
       " '578': 38588,\n",
       " 'ĠFrequency': 42389,\n",
       " 'Ġbenevolent': 43186,\n",
       " 'Ġsurvivors': 7149,\n",
       " 'Ġenrol': 23716,\n",
       " 'SHARE': 44624,\n",
       " 'Ġorchestra': 20642,\n",
       " 'ĠFernand': 26740,\n",
       " 'Ġblackout': 32010,\n",
       " 'ŃĶ': 49898,\n",
       " 'saf': 43425,\n",
       " '----': 44259,\n",
       " 'Ġaffiliate': 10515,\n",
       " 'Custom': 36962,\n",
       " 'éĹĺ': 50044,\n",
       " 'Ġdairy': 9616,\n",
       " 'Ġpip': 35209,\n",
       " 'Ġadoptive': 36755,\n",
       " 'è£': 49533,\n",
       " 'ãĥ¼ãĥ«': 49613,\n",
       " 'Ġencount': 49961,\n",
       " 'Ġvisas': 13570,\n",
       " 'ĠSystem': 5149,\n",
       " 'Ġrenov': 21476,\n",
       " 'Ġchuckled': 42827,\n",
       " 'ĠCC': 9841,\n",
       " 'Ġpublicity': 13698,\n",
       " 'Ġpopulated': 18269,\n",
       " 'Ġexh': 30345,\n",
       " 'Ġsabot': 37940,\n",
       " 'awatts': 23643,\n",
       " 'Ġvigilance': 35116,\n",
       " 'ĠLead': 14243,\n",
       " '580': 31663,\n",
       " 'Ġah': 23184,\n",
       " 'Ġbeta': 6212,\n",
       " 'Ġdeparture': 5824,\n",
       " 'Ġstakeholders': 7193,\n",
       " 'Ġbias': 9415,\n",
       " 'church': 23420,\n",
       " 'Ġplace': 317,\n",
       " 'xx': 29061,\n",
       " 'Ġcourtesy': 7676,\n",
       " 'ĠDiego': 3402,\n",
       " '®': 2840,\n",
       " 'Ġobserves': 34649,\n",
       " 'kered': 20093,\n",
       " 'Ġsurveillance': 5786,\n",
       " 'mc': 29297,\n",
       " 'behavior': 40397,\n",
       " 'ua': 4324,\n",
       " 'Ġeach': 349,\n",
       " 'Ġvibrations': 39074,\n",
       " 'it': 405,\n",
       " 'ĠSHOULD': 41814,\n",
       " 'ãĤ³': 49195,\n",
       " 'parency': 24470,\n",
       " 'Ġunivers': 35503,\n",
       " 'Fel': 39611,\n",
       " 'Ġmap': 5456,\n",
       " 'Ġtopped': 8319,\n",
       " 'Ind': 15248,\n",
       " 'Ġclaws': 41082,\n",
       " 'Ġreceptions': 15468,\n",
       " 'Ġbenchmarks': 22485,\n",
       " 'Ġdictator': 18529,\n",
       " 'Ġinsiders': 8169,\n",
       " 'Ġspots': 5284,\n",
       " '·': 18400,\n",
       " 'ĠBud': 11504,\n",
       " 'Ġidentified': 2006,\n",
       " 'ĠAbbey': 20481,\n",
       " '380': 19148,\n",
       " 'į': 8384,\n",
       " 'Ġvanished': 22147,\n",
       " 'onda': 11192,\n",
       " 'CL': 7454,\n",
       " 'ense': 9401,\n",
       " 'ĠPerformance': 10193,\n",
       " 'ĠEvents': 13634,\n",
       " 'rast': 16136,\n",
       " 'ao': 3853,\n",
       " 'pas': 28466,\n",
       " 'ĠEMP': 42384,\n",
       " 'Multiple': 43253,\n",
       " 'ĠPlex': 41707,\n",
       " 'Ġ28': 971,\n",
       " 'Ġreliance': 14197,\n",
       " 'de': 2794,\n",
       " 'Ġhorr': 48067,\n",
       " 'ĠBalance': 26193,\n",
       " 'ĠConsortium': 31693,\n",
       " 'Ġtemporal': 41853,\n",
       " 'Birth': 44728,\n",
       " 'ĠHawai': 32521,\n",
       " 'Ġmerger': 7394,\n",
       " 'is': 354,\n",
       " 'Allah': 45077,\n",
       " 'ggie': 30210,\n",
       " 'rified': 34115,\n",
       " 'ĠTap': 14624,\n",
       " 'Ġgenius': 16333,\n",
       " 'Ġdifferently': 8225,\n",
       " 'Miss': 22885,\n",
       " 'ĠMel': 4448,\n",
       " 'ĠDA': 9036,\n",
       " 'qua': 23869,\n",
       " 'dest': 31549,\n",
       " 'nikov': 32371,\n",
       " 'ĠSeems': 36420,\n",
       " 'ĠâľĶ': 45970,\n",
       " 'ĠTypical': 42673,\n",
       " 'gre': 28819,\n",
       " 'Ġrecognized': 4984,\n",
       " 'Ġmusicians': 8884,\n",
       " 'Ġprefix': 46622,\n",
       " 'ibu': 16343,\n",
       " 'ĠTe': 2941,\n",
       " 'ĠPvP': 47026,\n",
       " 'Ġcontemporary': 8708,\n",
       " 'ĠLow': 6207,\n",
       " 'brand': 11638,\n",
       " 'ĠElm': 21595,\n",
       " 'Ġvalve': 24423,\n",
       " 'Ġnecessity': 17331,\n",
       " 'ĠComponents': 39320,\n",
       " 'Ġinclusion': 9290,\n",
       " 'ĠGonzalez': 9670,\n",
       " 'ĠCheney': 30649,\n",
       " 'mer': 2089,\n",
       " 'Ġconcession': 17287,\n",
       " 'asonable': 44640,\n",
       " 'Includes': 48100,\n",
       " 'Ġ330': 21006,\n",
       " 'ĠImprovement': 26657,\n",
       " 'ohydrate': 47238,\n",
       " 'nn': 15688,\n",
       " 'iber': 9760,\n",
       " 'ĠBabylon': 41170,\n",
       " 'ĠPun': 14687,\n",
       " 'çĶ': 47873,\n",
       " 'ĠMathemat': 46828,\n",
       " 'anners': 32173,\n",
       " 'Ġen': 1177,\n",
       " 'Ġjails': 25559,\n",
       " '710': 36867,\n",
       " 'Ess': 38469,\n",
       " 'dos': 19202,\n",
       " 'Ġsummon': 33411,\n",
       " 'Ġreferees': 23811,\n",
       " 'Ġostr': 35902,\n",
       " 'Ġlike': 101,\n",
       " 'Ġmedium': 4761,\n",
       " 'Ġshall': 5658,\n",
       " 'ĠCat': 7641,\n",
       " 'Ġinsights': 8418,\n",
       " 'Ġenhanced': 9094,\n",
       " 'Ġcro': 11398,\n",
       " 'ĠDug': 15165,\n",
       " 'clear': 18763,\n",
       " 'Web': 27521,\n",
       " 'mond': 8876,\n",
       " 'ĠBenson': 20236,\n",
       " 'ĠRussian': 1083,\n",
       " 'ould': 25513,\n",
       " 'Ġmos': 25104,\n",
       " 'ĠProvider': 31314,\n",
       " 'vr': 37032,\n",
       " 'ĠBride': 37992,\n",
       " 'Ġwhipped': 23046,\n",
       " 'Ang': 25441,\n",
       " 'ĠPET': 21817,\n",
       " 'ĠReed': 7781,\n",
       " 'Ġknockout': 14647,\n",
       " 'ĠTales': 32419,\n",
       " 'Ġbag': 3298,\n",
       " 'Ġcav': 32426,\n",
       " 'ĠZelda': 35274,\n",
       " 'Alright': 45636,\n",
       " 'aughlin': 19326,\n",
       " 'Ġmarrying': 27372,\n",
       " 'Ġsoften': 28679,\n",
       " 'ĠSAS': 32143,\n",
       " 'Ġwiki': 47068,\n",
       " 'Ġ|--': 49812,\n",
       " 'ĠBone': 25719,\n",
       " 'pert': 11497,\n",
       " 'Ġprin': 27222,\n",
       " 'âĸĪâĸĪâĸĪâĸĪâĸĪâĸĪâĸĪâĸĪ': 49911,\n",
       " 'ĠIllust': 36599,\n",
       " 'ĠKes': 18987,\n",
       " 'Ġconsidered': 1687,\n",
       " 'Ġprosperity': 12536,\n",
       " 'ĠMedicare': 8999,\n",
       " 'ĠOy': 17311,\n",
       " 'ĠUltimate': 19687,\n",
       " 'soft': 24810,\n",
       " 'uner': 25131,\n",
       " 'Ġhit': 478,\n",
       " 'Golden': 31704,\n",
       " 'abbling': 37333,\n",
       " 'Ġeverlasting': 44256,\n",
       " 'Ġdives': 33201,\n",
       " 'Ġpuzzles': 31945,\n",
       " 'Ġsurrender': 13446,\n",
       " 'Ġauthorized': 8672,\n",
       " 'Grad': 44908,\n",
       " 'GGGGGGGG': 49901,\n",
       " 'Ġartery': 30404,\n",
       " 'Ġpissed': 34449,\n",
       " 'Ġgreatest': 3968,\n",
       " 'agents': 40784,\n",
       " 'Ġdispos': 22031,\n",
       " 'Ġcaption': 3747,\n",
       " 'Ġchanged': 1714,\n",
       " 'ointed': 26427,\n",
       " 'ĠDum': 16664,\n",
       " 'ĠAdvoc': 19227,\n",
       " 'gravity': 43687,\n",
       " 'ĠMeow': 46356,\n",
       " 'Ġstrongest': 8260,\n",
       " '352': 33674,\n",
       " 'ĠCorvette': 36276,\n",
       " 'Ġdevelopment': 709,\n",
       " 'ĠATP': 22020,\n",
       " 'ĠEsper': 35896,\n",
       " 'omial': 48866,\n",
       " '||||': 49859,\n",
       " 'ĠBBC': 3295,\n",
       " 'Most': 2895,\n",
       " 'ĠHam': 3600,\n",
       " 'Ġintegrity': 7066,\n",
       " 'Ġinclined': 19901,\n",
       " 'Sun': 8816,\n",
       " 'hero': 19034,\n",
       " 'Ġdependence': 20748,\n",
       " 'Series': 25166,\n",
       " 'Ġactivism': 16696,\n",
       " 'Ġspoilers': 17669,\n",
       " 'Ġaids': 25842,\n",
       " 'ĠNatural': 7278,\n",
       " 'ĠWeber': 15275,\n",
       " 'Ġsign': 1203,\n",
       " 'Ġsmash': 19005,\n",
       " 'Ġboon': 28240,\n",
       " 'Brend': 29178,\n",
       " 'Ġpins': 21321,\n",
       " 'Ġidentifiable': 35432,\n",
       " 'ĠTurtle': 35598,\n",
       " 'Ġbean': 23216,\n",
       " 'gang': 21251,\n",
       " 'Ġtipping': 22737,\n",
       " 'Ġmistake': 5021,\n",
       " 'ĠRome': 8947,\n",
       " 'Ġway': 169,\n",
       " 'comings': 47657,\n",
       " 'ĠKev': 30762,\n",
       " 'Ġconvinc': 38753,\n",
       " 'ĠWak': 27540,\n",
       " 'Ġquarters': 5666,\n",
       " 'Ġprotected': 4371,\n",
       " 'ĠGS': 19580,\n",
       " 'Ġplacebo': 26231,\n",
       " 'Ġdrilling': 7802,\n",
       " 'Ġfurnace': 36574,\n",
       " 'Ġtravelling': 7290,\n",
       " 'Ġlunar': 21641,\n",
       " 'EStream': 50225,\n",
       " 'ĠPVC': 33061,\n",
       " 'scientific': 43723,\n",
       " 'Ġpull': 2999,\n",
       " 'ĠIndigenous': 9237,\n",
       " 'Ġnormally': 6329,\n",
       " 'aries': 5119,\n",
       " 'Ġmol': 13200,\n",
       " 'ĠBing': 14365,\n",
       " 'ĠWeld': 30410,\n",
       " 'ĠTBD': 37242,\n",
       " 'unker': 37083,\n",
       " 'mL': 43619,\n",
       " 'astrous': 45855,\n",
       " 'ĠMercenary': 49219,\n",
       " 'Ġfavored': 15411,\n",
       " 'ĠEducation': 3061,\n",
       " 'Ġvou': 45183,\n",
       " 'engine': 23403,\n",
       " 'Ġsand': 6255,\n",
       " 'atari': 22920,\n",
       " 'ress': 5224,\n",
       " 'Ġvillagers': 17603,\n",
       " 'Ġovercrowd': 22002,\n",
       " 'Ġomn': 32442,\n",
       " 'senal': 49872,\n",
       " 'ĠJesse': 10716,\n",
       " 'ĠRem': 8022,\n",
       " 'Ġcourageous': 24219,\n",
       " 'Ġcontinu': 27351,\n",
       " 'Ġhorses': 8087,\n",
       " 'Ġbetter': 357,\n",
       " 'ĠBE': 6362,\n",
       " 'ĠRy': 11861,\n",
       " 'ahead': 16905,\n",
       " 'Ġanalyse': 28399,\n",
       " 'Ġtelephone': 7377,\n",
       " 'themed': 14694,\n",
       " 'eper': 30816,\n",
       " 'ĠPilgrim': 39921,\n",
       " 'Ġplayback': 20083,\n",
       " 'ĠRanked': 41915,\n",
       " 'Ġturbulence': 28742,\n",
       " 'strength': 33687,\n",
       " 'Anna': 35242,\n",
       " 'Ġoutraged': 22339,\n",
       " 'ĠKin': 12823,\n",
       " 'ĠSentinel': 12508,\n",
       " 'Ġordinances': 31840,\n",
       " 'osuke': 46731,\n",
       " 'Ġbullish': 12115,\n",
       " 'mitting': 37740,\n",
       " 'ĠFind': 6449,\n",
       " 'aur': 8616,\n",
       " 'Ġminorities': 12568,\n",
       " 'Ġcries': 25355,\n",
       " 'Ġsmear': 27337,\n",
       " 'atron': 44404,\n",
       " 'Ġprotagonist': 24587,\n",
       " 'ĠRoose': 47399,\n",
       " 'Ġdinner': 3630,\n",
       " 'Ġknife': 7023,\n",
       " 'Ġ179': 29189,\n",
       " 'Ġtopic': 5674,\n",
       " 'Ġranging': 6272,\n",
       " 'Ġconfidential': 12143,\n",
       " 'Ġlongtime': 5878,\n",
       " '709': 38139,\n",
       " 'igil': 40252,\n",
       " 'ĠSpring': 5519,\n",
       " 'ĠAssist': 31070,\n",
       " '777': 33416,\n",
       " 'Ġfps': 43690,\n",
       " 'Ġenrollment': 12510,\n",
       " 'Ġcreek': 23954,\n",
       " 'ĠJuan': 7056,\n",
       " 'Ġclean': 2382,\n",
       " 'Ġlap': 8040,\n",
       " 'Ġkh': 16447,\n",
       " 'asley': 22857,\n",
       " 'ĠScale': 33256,\n",
       " 'Ġslapping': 33640,\n",
       " 'Ġãģ': 48897,\n",
       " 'ĠLie': 17974,\n",
       " 'ples': 12349,\n",
       " 'ĠSeahawks': 9409,\n",
       " 'Ġfundraising': 8023,\n",
       " 'ĠAzerbaijan': 12097,\n",
       " 'Ġscreenings': 20682,\n",
       " 'oine': 21012,\n",
       " 'ĠLol': 36293,\n",
       " 'Ġsquats': 43014,\n",
       " 'Ġrav': 25283,\n",
       " 'iency': 33625,\n",
       " 'eli': 20274,\n",
       " 'andem': 43171,\n",
       " 'Ġspelled': 29341,\n",
       " 'Ġted': 44817,\n",
       " 'joining': 45683,\n",
       " 'Ġneed': 240,\n",
       " 'ĠIron': 9940,\n",
       " 'Ġsurrendered': 16313,\n",
       " 'eway': 10564,\n",
       " 'ĠPing': 27161,\n",
       " 'Ġferocious': 31429,\n",
       " 'Micro': 39599,\n",
       " 'Ġlact': 36149,\n",
       " 'Ġsalesman': 32656,\n",
       " 'Ġoxid': 44322,\n",
       " 'KS': 18307,\n",
       " 'olph': 32489,\n",
       " 'Ġom': 16780,\n",
       " 'Ġeminent': 33015,\n",
       " 'elsen': 35573,\n",
       " 'dim': 37152,\n",
       " 'utory': 41930,\n",
       " 'ĠMerkel': 6580,\n",
       " 'Ġhas': 34,\n",
       " 'Ġloss': 872,\n",
       " 'ĠBehind': 17285,\n",
       " 'Ġabstract': 20372,\n",
       " 'Ġoverd': 21985,\n",
       " 'Ġguys': 1669,\n",
       " 'Ġequate': 36316,\n",
       " 'ĠVT': 37599,\n",
       " 'isted': 15346,\n",
       " 'Ġcapac': 30862,\n",
       " 'Ġimport': 6595,\n",
       " 'aer': 27075,\n",
       " 'ĠAlert': 14718,\n",
       " 'Ġtrapping': 33634,\n",
       " 'Ġknown': 684,\n",
       " 'ĠScenes': 40267,\n",
       " 'Alice': 43714,\n",
       " 'Ġswayed': 34828,\n",
       " 'Unit': 43980,\n",
       " 'pha': 44641,\n",
       " 'thereum': 45507,\n",
       " 'advertising': 46429,\n",
       " 'Ġfound': 303,\n",
       " 'ĠJord': 18340,\n",
       " 'Ġchange': 464,\n",
       " 'Ġga': 26848,\n",
       " 'Ġpursue': 5445,\n",
       " 'ĠRhode': 12374,\n",
       " 'verse': 15189,\n",
       " 'ĠFlan': 21142,\n",
       " 'ĠCinderella': 34800,\n",
       " 'omical': 40421,\n",
       " 'Ġpolit': 10635,\n",
       " 'Ġdri': 13911,\n",
       " 'iversity': 31104,\n",
       " 'Ġdownt': 29407,\n",
       " 'Ġintervals': 29087,\n",
       " 'Ġexternally': 38826,\n",
       " 'cember': 47153,\n",
       " 'osponsors': 48181,\n",
       " 'ĠVirtual': 21741,\n",
       " '\\\\.': 49410,\n",
       " 'Ġdwar': 35535,\n",
       " 'phis': 42377,\n",
       " 'Ġsuch': 215,\n",
       " 'ĠDrew': 8238,\n",
       " 'ĠReynolds': 9295,\n",
       " 'ĠDirect': 10480,\n",
       " 'Ġmud': 10511,\n",
       " 'killers': 27621,\n",
       " 'ĠReached': 43540,\n",
       " 'readable': 46753,\n",
       " 'Ġ1948': 21053,\n",
       " 'lishes': 46808,\n",
       " 'Sm': 23996,\n",
       " 'ĠCorona': 28774,\n",
       " 'Ġadditions': 14886,\n",
       " 'ĠNHL': 4693,\n",
       " 'ĠLeica': 43247,\n",
       " 'Ġpriorities': 7532,\n",
       " 'Play': 20780,\n",
       " 'rendered': 47392,\n",
       " 'Ġtitle': 1270,\n",
       " 'Ġphysique': 29543,\n",
       " 'Ġfruits': 12849,\n",
       " 'ĠClayton': 13616,\n",
       " 'Aaron': 29934,\n",
       " 'ĠBeware': 41926,\n",
       " 'Ġtape': 7898,\n",
       " 'ĠIncredible': 36665,\n",
       " 'Fill': 46448,\n",
       " '$$$$': 49193,\n",
       " 'Face': 34892,\n",
       " 'Ġpupp': 32986,\n",
       " 'Hung': 41416,\n",
       " 'ĠEastern': 3877,\n",
       " 'Ġ62': 5356,\n",
       " 'ĠDion': 18030,\n",
       " 'Ġinsol': 23799,\n",
       " 'Ġproced': 31902,\n",
       " 'Ġswirl': 33361,\n",
       " 'Ġregener': 36445,\n",
       " 'Ġinvestigate': 4830,\n",
       " 'ĠGarner': 21059,\n",
       " 'ĠMaurit': 26214,\n",
       " 'Were': 43822,\n",
       " 'Ġtyrant': 43959,\n",
       " 'Privacy': 46591,\n",
       " 'Ġè£ı': 50073,\n",
       " 'successfully': 46958,\n",
       " 'ossip': 35748,\n",
       " 'Ġushered': 30155,\n",
       " 'Ġmasc': 40000,\n",
       " 'MODE': 49523,\n",
       " 'ayed': 12198,\n",
       " 'Ġtar': 20318,\n",
       " 'ĠSpani': 14703,\n",
       " 'Ġcloning': 44246,\n",
       " 'Ġoccasionally': 10930,\n",
       " 'ĠStore': 7248,\n",
       " 'ĠDO': 14010,\n",
       " 'upload': 47571,\n",
       " 'ĠThanks': 4557,\n",
       " 'bage': 36772,\n",
       " 'ĠFighting': 18563,\n",
       " 'Ġpredicted': 6126,\n",
       " 'Ġtele': 8327,\n",
       " 'aum': 22317,\n",
       " 'Ġsmoother': 31675,\n",
       " 'ĠMer': 4213,\n",
       " 'orthy': 32894,\n",
       " 'Ġfearsome': 39185,\n",
       " 'ĠInquisition': 47237,\n",
       " 'Ġangle': 11792,\n",
       " 'PLIC': 44597,\n",
       " 'Ġclar': 17691,\n",
       " 'Ġcalming': 31220,\n",
       " 'ranch': 38436,\n",
       " 'ĠTart': 32724,\n",
       " 'ezvous': 42057,\n",
       " 'ĠCuba': 8455,\n",
       " 'Ġqualities': 13408,\n",
       " 'ĠDani': 18700,\n",
       " 'edge': 14724,\n",
       " 'Compar': 48080,\n",
       " 'Ġjeans': 10844,\n",
       " 'ĠC': 230,\n",
       " 'NSA': 42219,\n",
       " 'ggle': 45619,\n",
       " 'ĠWolverine': 34594,\n",
       " 'Ġrud': 39945,\n",
       " 'Ġdownloadable': 40908,\n",
       " 'Fer': 42594,\n",
       " 'Ġ..........': 29462,\n",
       " 'Sem': 37504,\n",
       " 'Ġtrusted': 10128,\n",
       " 'Ġfl': 2342,\n",
       " 'Ġfutures': 5060,\n",
       " 'ĠBloomberg': 6686,\n",
       " 'Ġtwin': 9544,\n",
       " 'Ġjurisdictions': 17607,\n",
       " 'sector': 18658,\n",
       " 'Control': 43405,\n",
       " 'Ġcollided': 15268,\n",
       " 'ĠTumblr': 9494,\n",
       " 'Ġrestoring': 17361,\n",
       " 'Read': 25439,\n",
       " 'Ġgram': 25187,\n",
       " 'Ġlearnt': 13973,\n",
       " 'ĠPowered': 37910,\n",
       " 'hops': 39294,\n",
       " 'Ur': 38046,\n",
       " 'Fr': 29220,\n",
       " 'perture': 46782,\n",
       " 'Live': 16549,\n",
       " 'romptu': 30467,\n",
       " 'nect': 45114,\n",
       " 'ossom': 35728,\n",
       " 'docs': 44071,\n",
       " 'css': 48408,\n",
       " 'ĠRegion': 6131,\n",
       " 'Ġbarred': 12678,\n",
       " 'dep': 17272,\n",
       " 'ĠJordan': 2875,\n",
       " 'ĠShap': 35069,\n",
       " 'ĠMassive': 33859,\n",
       " 'vir': 28641,\n",
       " 'atsuki': 47497,\n",
       " 'Ġhistor': 23671,\n",
       " 'Ġdecline': 2991,\n",
       " 'Ġrecounted': 21994,\n",
       " 'ĠEthiopian': 24128,\n",
       " 'Ġweap': 49772,\n",
       " 'ĠRoth': 13880,\n",
       " 'erton': 14347,\n",
       " 'Ġstatutory': 17947,\n",
       " 'Ġlaser': 13443,\n",
       " 'Ġswimming': 7358,\n",
       " 'Ġstream': 4615,\n",
       " 'ĠBryan': 7573,\n",
       " 'Ġph': 7843,\n",
       " 'ĠFounding': 37884,\n",
       " 'Ġvom': 23953,\n",
       " 'Teen': 39556,\n",
       " 'ifix': 48121,\n",
       " 'Ġib': 34154,\n",
       " 'Ġcontem': 44634,\n",
       " 'imil': 27550,\n",
       " '%': 207,\n",
       " 'Ġthrough': 149,\n",
       " 'ĠYEAR': 22554,\n",
       " 'Ġshadow': 9099,\n",
       " 'Ġ262': 35523,\n",
       " 'ĠJunior': 6843,\n",
       " 'ĠYamaha': 25297,\n",
       " 'Ġthreshold': 11543,\n",
       " 'Apart': 41041,\n",
       " 'travel': 28881,\n",
       " 'Ġrice': 7666,\n",
       " 'Ġkitten': 35292,\n",
       " '2001': 33185,\n",
       " 'Gi': 40405,\n",
       " 'BSD': 46540,\n",
       " 'ĠLumpur': 19193,\n",
       " 'ggles': 43312,\n",
       " 'ĠForbidden': 45076,\n",
       " 'Stars': 25808,\n",
       " 'Ġdriver': 1393,\n",
       " 'Ġripped': 12256,\n",
       " 'now': 8310,\n",
       " 'Ġdoct': 40975,\n",
       " 'Ġdefinitively': 35144,\n",
       " 'ak': 677,\n",
       " 'ĠWARNING': 16774,\n",
       " 'Ġnut': 17145,\n",
       " 'Ġcovered': 2913,\n",
       " 'ĠDisk': 39770,\n",
       " '877': 27806,\n",
       " 'ĠVIDEOS': 21671,\n",
       " 'ĠNaz': 14618,\n",
       " 'ihar': 33145,\n",
       " 'Ġnonexistent': 38854,\n",
       " 'Ġgloss': 27293,\n",
       " 'Ġsilhouette': 33585,\n",
       " 'Ġremission': 37026,\n",
       " 'ĠCustom': 16583,\n",
       " 'Ġcollaboration': 4918,\n",
       " 'Ġblessing': 14164,\n",
       " 'Ġhealing': 11759,\n",
       " 'ĠSop': 31403,\n",
       " 'Ġresusc': 31439,\n",
       " 'Ġshin': 38802,\n",
       " 'greg': 40745,\n",
       " 'Ġdiff': 25871,\n",
       " 'Round': 41061,\n",
       " 'bly': 27104,\n",
       " 'Ġglobe': 7183,\n",
       " 'ople': 28948,\n",
       " 'clip': 46413,\n",
       " 'ablishment': 47960,\n",
       " 'ĠTimbers': 34303,\n",
       " 'ischer': 32949,\n",
       " '956': 41632,\n",
       " 'Ġreplication': 42780,\n",
       " 'ĠFt': 34434,\n",
       " 'ĠKob': 18131,\n",
       " 'ula': 5571,\n",
       " 'ĠHeck': 26171,\n",
       " 'Ġ398': 42453,\n",
       " 'Ġ184': 29546,\n",
       " 'ĠBezos': 17767,\n",
       " 'ĠEmb': 6133,\n",
       " 'ĠSupplement': 44484,\n",
       " 'chemist': 48323,\n",
       " 'irect': 38806,\n",
       " 'ĠFirefly': 45060,\n",
       " 'ĠMcConnell': 8282,\n",
       " 'Ġshareholder': 7176,\n",
       " 'Ġjacket': 8443,\n",
       " 'ĠTrinity': 13544,\n",
       " 'Â¨': 36348,\n",
       " 'Ġprosecutors': 3659,\n",
       " 'Ġpictured': 7092,\n",
       " 'Ġbrilliant': 6967,\n",
       " 'Ġ1985': 12497,\n",
       " 'TM': 14386,\n",
       " 'Ġacadem': 32851,\n",
       " 'Ġwhy': 596,\n",
       " 'Ġears': 12137,\n",
       " 'ĠCrystal': 9793,\n",
       " 'DEP': 41372,\n",
       " 'V': 846,\n",
       " 'Ġcities': 1947,\n",
       " 'gs': 15453,\n",
       " 'Ġ.': 479,\n",
       " 'ĠSwitch': 11171,\n",
       " 'ĠCarrie': 13622,\n",
       " 'ĠBringing': 31767,\n",
       " 'ĠLouisiana': 5993,\n",
       " 'Nov': 18187,\n",
       " 'diff': 32278,\n",
       " 'noxious': 48165,\n",
       " 'ĠEcc': 29488,\n",
       " 'ĠHon': 8768,\n",
       " ']),': 48677,\n",
       " 'Ġ1926': 36721,\n",
       " 'vich': 31073,\n",
       " 'ĠBolshevik': 46137,\n",
       " 'wart': 21660,\n",
       " '×Ĳ': 48295,\n",
       " 'Ġpositions': 2452,\n",
       " 'Brien': 9814,\n",
       " 'Ġfries': 22391,\n",
       " 'Ġrelat': 34265,\n",
       " \"''.\": 41667,\n",
       " 'Group': 25976,\n",
       " 'ĠIgnore': 45090,\n",
       " 'ĠEliot': 37474,\n",
       " 'Ġgoing': 164,\n",
       " 'Ġdamage': 1880,\n",
       " 'Ġhomemade': 17798,\n",
       " 'Ġretrospective': 33777,\n",
       " 'Ġstabil': 12964,\n",
       " 'ĠSomething': 13676,\n",
       " 'ĠRebecca': 10808,\n",
       " 'ĠLaure': 17263,\n",
       " 'half': 4809,\n",
       " 'Ġcarnage': 28161,\n",
       " 'Tele': 41854,\n",
       " 'vae': 45897,\n",
       " 'QU': 15513,\n",
       " 'Ġcontroversy': 6170,\n",
       " 'Ġtaxable': 26475,\n",
       " 'Ġav': 6402,\n",
       " 'suff': 30403,\n",
       " 'ĠWins': 19460,\n",
       " 'Company': 21960,\n",
       " 'ĠLouie': 34488,\n",
       " 'Ġpercentile': 42097,\n",
       " 'Ġassum': 46685,\n",
       " 'ĠBolt': 17001,\n",
       " 'ANS': 6557,\n",
       " 'sized': 8407,\n",
       " 'Ġclassroom': 8171,\n",
       " 'ĠPhotos': 10878,\n",
       " 'Ġcrocod': 28736,\n",
       " '¾': 4726,\n",
       " 'Ġslams': 30506,\n",
       " 'Ġthreatens': 13546,\n",
       " 'urance': 12590,\n",
       " 'Ġ372': 41717,\n",
       " 'Ġflagship': 8589,\n",
       " 'Ġrepr': 25874,\n",
       " 'Ġnotion': 9976,\n",
       " 'ĠRepe': 32021,\n",
       " 'ĠATT': 28866,\n",
       " 'ĠCause': 24197,\n",
       " 'rm': 22900,\n",
       " 'Ġhistorian': 17089,\n",
       " 'People': 4763,\n",
       " 'Ġreductions': 14138,\n",
       " 'Ġbol': 24724,\n",
       " 'AGE': 14426,\n",
       " 'Ġterminals': 20531,\n",
       " 'ĠGrind': 26182,\n",
       " 'Ġpen': 7670,\n",
       " 'Ġskull': 19444,\n",
       " 'ĠRays': 15703,\n",
       " 'roman': 32562,\n",
       " 'ĠFasc': 41815,\n",
       " 'ĠBreakfast': 18799,\n",
       " 'ĠThro': 43136,\n",
       " 'ĠRestaur': 26210,\n",
       " 'Ġagencies': 2244,\n",
       " 'ourmet': 23299,\n",
       " 'Ġangel': 20285,\n",
       " 'ĠStraight': 30020,\n",
       " 'ĠOss': 33613,\n",
       " 'ateful': 39264,\n",
       " 'competitive': 29288,\n",
       " 'igen': 8554,\n",
       " 'Ġarguing': 7594,\n",
       " 'ADD': 37705,\n",
       " 'ĠDispatch': 29517,\n",
       " 'Merc': 43965,\n",
       " 'social': 19027,\n",
       " 'Ghost': 38856,\n",
       " 'False': 46659,\n",
       " 'Ġsound': 2369,\n",
       " 'DERR': 49809,\n",
       " 'ves': 3677,\n",
       " 'Ġreinvest': 24090,\n",
       " 'ĠExtensions': 46942,\n",
       " 'Ġdetermines': 23483,\n",
       " 'Ġseverity': 17866,\n",
       " 'Ġcoins': 15750,\n",
       " 'Ġattachment': 26350,\n",
       " 'yon': 21743,\n",
       " 'Ġrecorder': 32017,\n",
       " 'Ġrituals': 27830,\n",
       " 'conscious': 22035,\n",
       " 'ĠDam': 8234,\n",
       " 'imester': 38417,\n",
       " 'ĠWater': 3201,\n",
       " 'cil': 17286,\n",
       " 'Ġassets': 1781,\n",
       " 'Ġharmony': 20299,\n",
       " 'Ġdecisive': 12703,\n",
       " 'Ġprogrammer': 38988,\n",
       " 'ocaust': 43328,\n",
       " 'Ġpivotal': 14125,\n",
       " 'ocol': 23558,\n",
       " 'dozen': 28680,\n",
       " 'nuclear': 31242,\n",
       " 'Ġstead': 23789,\n",
       " 'ĠEffective': 33355,\n",
       " 'Ġcook': 7142,\n",
       " 'fitted': 29043,\n",
       " 'none': 39763,\n",
       " 'ĠSTOR': 17399,\n",
       " 'ĠIncarn': 46088,\n",
       " 'shared': 42502,\n",
       " 'ĠPrelude': 46387,\n",
       " 'minent': 42522,\n",
       " 'Ġ1862': 44695,\n",
       " 'itle': 46398,\n",
       " 'Ġmid': 1084,\n",
       " 'Ġgolf': 3524,\n",
       " 'quist': 21974,\n",
       " 'Ġvir': 26354,\n",
       " 'children': 15097,\n",
       " 'Ġchanting': 21122,\n",
       " 'Ġcheesy': 36331,\n",
       " 'Ġmodem': 38533,\n",
       " 'Grid': 41831,\n",
       " 'bedroom': 15112,\n",
       " '------------': 46156,\n",
       " 'ĠNiet': 47036,\n",
       " 'arch': 13161,\n",
       " 'Initialized': 50026,\n",
       " 'ãģ': 46311,\n",
       " 'ĠProbably': 20847,\n",
       " 'plementation': 45661,\n",
       " 'ĠPacific': 3073,\n",
       " 'ĠTrevor': 10251,\n",
       " 'ĠGibson': 9909,\n",
       " 'ayer': 19777,\n",
       " 'Ġsenseless': 28846,\n",
       " 'astically': 41936,\n",
       " 'Sch': 18077,\n",
       " 'Ġ\\\\': 44128,\n",
       " 'shaped': 15660,\n",
       " 'Ġexpresses': 15994,\n",
       " 'Ġmold': 16140,\n",
       " 'ĠCompany': 1260,\n",
       " 'ibly': 19031,\n",
       " 'Ġrunoff': 17020,\n",
       " 'ĠMÃ©': 26617,\n",
       " 'IVERS': 30530,\n",
       " 'Create': 44758,\n",
       " 'ĠReloaded': 47784,\n",
       " 'ĠBangalore': 20071,\n",
       " 'ĠRunes': 49421,\n",
       " 'ĨĴ': 49823,\n",
       " 'ĠCal': 2912,\n",
       " 'HY': 32288,\n",
       " 'ingle': 18768,\n",
       " 'ĠZucker': 37809,\n",
       " 'ĠNate': 13320,\n",
       " '340': 24334,\n",
       " 'Chart': 25136,\n",
       " 'ĠElectro': 32893,\n",
       " 'Ġvain': 25876,\n",
       " 'uint': 47157,\n",
       " 'Ġnoodles': 27972,\n",
       " 'ĠInterior': 8867,\n",
       " 'Ġ2018': 199,\n",
       " 'ĠPetroleum': 9022,\n",
       " 'Ġglobal': 720,\n",
       " 'Ġleaning': 19146,\n",
       " 'ĠReef': 27523,\n",
       " 'Ġsignifies': 39427,\n",
       " 'Ġgoodwill': 20061,\n",
       " 'ĠSTR': 20857,\n",
       " 'Ġhydro': 13575,\n",
       " 'different': 37251,\n",
       " 'Ġinterpersonal': 40226,\n",
       " 'forum': 44424,\n",
       " ...}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c6c35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dc146b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.3% 65.0/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.5% 231.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 69.5% 261.2/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ac9be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_matrices(A, B, A_prime):\n",
    "    X = np.linalg.lstsq(A, B, rcond=None)[0]\n",
    "    B_prime = np.dot(A_prime, X)\n",
    "    return B_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f13838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_matrices(A, B, A_prime):\n",
    "    U_A, S_A, V_A_T = np.linalg.svd(A)\n",
    "    U_B, S_B, V_B_T = np.linalg.svd(B)\n",
    "    T = np.dot(U_B, np.transpose(V_A_T))\n",
    "    B_prime = np.dot(np.dot(U_B, np.diag(S_B)), np.dot(np.transpose(V_A_T), A_prime))\n",
    "    return B_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e1825914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_matrix(A, A_prime, B):\n",
    "    # Perform SVD on A\n",
    "    U, S, VT = np.linalg.svd(A)\n",
    "\n",
    "    # Compute the matrix P\n",
    "    P = np.matmul(U, np.diag(S))\n",
    "\n",
    "    # Multiply the transpose of A_prime by P to obtain B_prime\n",
    "    B_prime = np.matmul(P.T, A_prime.T)\n",
    "\n",
    "    return B_prime.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4ac1a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((3, 4))\n",
    "a_ = np.ones((3, 7)) * 5\n",
    "b = np.ones((9, 4))\n",
    "b_ = transform_matrices(a, a_, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e0e597ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5., 5., 5., 5., 5.],\n",
       "       [5., 5., 5., 5., 5., 5., 5.],\n",
       "       [5., 5., 5., 5., 5., 5., 5.]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dab908c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.shared.weight.data[0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d20be94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "a_ = []\n",
    "\n",
    "a_index = []\n",
    "b_index = []\n",
    "\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    word = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens([i])).lower().strip()\n",
    "    if word in vectors.key_to_index:\n",
    "        a_.append(vectors.vectors[int(vectors.key_to_index[word])])\n",
    "        a.append(model.model.shared.weight.data[i].numpy())\n",
    "        a_index.append(i)\n",
    "    else:\n",
    "        b_index.append(i)\n",
    "        b.append(model.model.shared.weight.data[i].numpy())\n",
    "        \n",
    "a = np.stack(a)\n",
    "b = np.stack(b)\n",
    "a_ = np.stack(a_)\n",
    "b_ = transform_matrices(a, a_, b)\n",
    "\n",
    "a_ = dict(zip(a_index, a_))\n",
    "b_ = dict(zip(b_index, b_))\n",
    "\n",
    "a_b_ = []\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    if i in a_:\n",
    "        a_b_.append(a_[i])\n",
    "    elif i in b_:\n",
    "        a_b_.append(b_[i])\n",
    "        \n",
    "a_b_ = np.stack(a_b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "91225bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50265, 300)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_b_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b384d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7173, 1024)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "322eb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.Embedding(50265, 300, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c4d1fabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0810,  0.3480, -0.1427,  ...,  0.1390,  0.3517, -0.2222],\n",
       "        [-0.0247,  0.0020,  0.0116,  ..., -0.0403,  0.1213, -0.0574],\n",
       "        [ 0.2418, -0.2084,  0.1825,  ...,  0.0107, -0.0044, -0.0240],\n",
       "        ...,\n",
       "        [ 0.0090,  0.0288,  0.0144,  ...,  0.0819, -0.0050, -0.1152],\n",
       "        [ 0.0008, -0.0063, -0.0616,  ..., -0.0085,  0.1387, -0.0041],\n",
       "        [-0.2786, -0.1212,  0.1806,  ...,  0.1673,  0.1719, -0.2255]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.data.copy_(torch.from_numpy(a_b_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "244608d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._api import register_model, Weights, WeightsEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "47875ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'WeightsEnum'>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WeightsEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "192596d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.alexnet import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "daa178df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save( m.state_dict(), 'Embedding-RobertaTokenizer-glove-wiki-gigaword-300.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259fa2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartConfig, BartModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aff2397",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my dog is cute\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy cute dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m outputs\n",
      "File \u001b[0;32m~/working/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/working/or/seq2seq-ngram/lava/modeling_lava.py:67\u001b[0m, in \u001b[0;36mLavaModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MaskedLMOutput:\n\u001b[0;32m---> 67\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m()\n\u001b[1;32m     68\u001b[0m     decoder_attention_mask \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     70\u001b[0m     decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m     71\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     72\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'long'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "# model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"my cute dog.\", return_tensors = 'pt')\n",
    "outputs = model(**inputs, labels = labels)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7e91f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1024])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "951f06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, PreTrainedModel, AutoModelForSeq2SeqLM, EncoderDecoderConfig, EncoderDecoderModel, logging, RobertaConfig, BartConfig, BartForConditionalGeneration, BartForQuestionAnswering\n",
    "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d45e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {0: 'LABEL_0', 1: 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from lava import LavaModel\n",
    "model = LavaModel.from_lava_pretrained('roberta-base', 'facebook/bart-base').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4138033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"my cute dog.\", return_tensors = 'pt')\n",
    "# outputs = model(inputs['input_ids'].cuda(), attention_mask = inputs['attention_mask'].cuda(), labels = labels['input_ids'].cuda())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f3b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LavaModel.from_lava_pretrained('roberta-base', 'facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7b7af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b6406d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1a505af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0114, -0.0169, -0.0184,  ..., -0.0131, -0.0043, -0.0053],\n",
       "        [ 0.0842, -0.0389,  0.0096,  ...,  0.0583,  0.0082,  0.0357]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels = 2\n",
    "lm_head = model.lm_head\n",
    "model.lm_head = torch.nn.Linear(in_features=lm_head.in_features, out_features=2, bias=True)\n",
    "model.lm_head.weight.data.copy_(lm_head.weight.data[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064ed1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1, bias=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fe917ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (50265) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/working/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/working/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1468\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1469\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1470\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/working/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2722\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2722\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2730\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/working/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/working/anaconda3/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1393\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1374\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1375\u001b[0m     input_ids,\n\u001b[1;32m   1376\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1390\u001b[0m )\n\u001b[1;32m   1392\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1393\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlm_logits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_logits_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (50265) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model.generate(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9eba0316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
