# -*- coding: utf-8 -*-
"""Roberta with ROUGELoss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xiXKU0DdS2dpvmkGzF76IeKzS9giJA57
"""

# #@title drive._mount
# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools
# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
# !apt-get update -qq 2>&1 > /dev/null
# !apt-get -y install -qq google-drive-ocamlfuse fuse
# from google.colab import auth
# auth.authenticate_user()
# from oauth2client.client import GoogleCredentials
# creds = GoogleCredentials.get_application_default()
# import getpass
# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
# vcode = getpass.getpass()
# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}
# %cd /content
# !mkdir drive
# %cd drive
# !mkdir MyDrive
# %cd ..
# %cd ..
# !google-drive-ocamlfuse /content/drive/MyDrive

# !rm -rf index; git clone https://github.com/cestwc/index.git

# from index.puller import pull_and_unzip
# pull_and_unzip('cnn_dailymail_snippet_tokenized')

# !pip install transformers >/dev/null
# !pip install datasets >/dev/null
# !rm -rf Differentiable-ROUGE-score; git clone https://github.com/cestwc/Differentiable-ROUGE-score.git

import sys
sys.path.append('Differentiable-ROUGE-score')
from trainer import RougeTrainer
# from losses import ROUGELoss
# sys.path.pop()

from datasets import load_dataset, load_from_disk

cnn_dailymail = load_dataset('ccdv/cnn_dailymail', '3.0.0')

from transformers import RobertaTokenizerFast, RobertaForMaskedLM
import torch

tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')
model = RobertaForMaskedLM.from_pretrained('roberta-base')

def tokenize(e):
    article = tokenizer(e['article'], max_length=512, truncation=True, padding = 'max_length')
    article['labels'] = tokenizer(e['highlights'], max_length=128, truncation=True, padding = 'max_length')['input_ids']
    
    return article

useful = lambda x: (len(x['article']) > len(x['highlights']) + 500) and len(x['highlights'].split()) > 20

cnn_dailymail = cnn_dailymail.filter(useful)

cnn_dailymail_tokenized = cnn_dailymail.map(tokenize, batched=True)

cnn_dailymail_tokenized.set_format(type='torch', columns=['input_ids','labels', 'attention_mask'])

print(f"Number of training examples: {len(cnn_dailymail_tokenized['train'])}")
print(f"Number of validation examples: {len(cnn_dailymail_tokenized['validation'])}")
print(f"Number of testing examples: {len(cnn_dailymail_tokenized['test'])}")

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The summarizer has {count_parameters(model):,} trainable parameters')

# drivePath = '/content/drive/MyDrive/Colab Notebooks/orange/'
# drivePath = ''

from transformers import Trainer, TrainingArguments

from torch import nn
from transformers import Trainer

from losses import ROUGELoss

training_args = TrainingArguments(
    output_dir= 'Roberta2',
    overwrite_output_dir=True,
    num_train_epochs=1,
    max_steps = 280_000,
    per_device_train_batch_size=4,
    save_steps=5_000,
    save_total_limit=20,
    prediction_loss_only=True,
    # learning_rate=3e-4,
    # logging_steps = 2
)

trainer = RougeTrainer(
    model = model,
    args = training_args,
    train_dataset = cnn_dailymail_tokenized['train'].shuffle(1234),
    eval_dataset = cnn_dailymail_tokenized['validation'],
    # data_collator = collate,
)

trainer.train(resume_from_checkpoint = False)

# %cd /content
# !rm -rf roberta-base-quater; git clone https://cestwc:Cest2021@huggingface.co/cestwc/roberta-base-quater

# trainer.save_model("./roberta-base-quater")
# tokenizer.save_pretrained("./roberta-base-quater")

# ## 5. Share your model ðŸŽ‰

# !sudo apt-get install git-lfs > /dev/null
# %cd /content/roberta-base-quater
# !git lfs install > /dev/null
# %cd /content

# model.push_to_hub("roberta-base-quater", use_auth_token='api_jRSyoExHrLUwsnBeLIUBJLgsCCJFRQlNgv')

trainer.evaluate()

# cnn_dailymail_tokenized['test'].reset_format()
# # test_data = cnn_dailymail_tokenized['train'].map(lambda x: tokenize(x), batched=True)

# sample = cnn_dailymail_tokenized['train'][0]
# sample["input_ids"].shape

# model.eval()

# output_sequences = model(
#     input_ids=sample["input_ids"].to(model.device).unsqueeze(0),
#     attention_mask=sample["attention_mask"].to(model.device).unsqueeze(0),
#     # do_sample=False,  # disable sampling to test if batching affects output
# )[0].argmax(2)

# sample["input_ids"]

# print(tokenizer.decode(output_sequences[0], skip_special_tokens=True), '\n')

# # tokenizer.padding_side = "left"

# i = 100
# inputs = tokenizer(cnn_dailymail_tokenized['test'][i]['article'], return_tensors="pt", padding=True)

# output_sequences = model.generate(
#     input_ids=inputs["input_ids"].to(model.device),
#     attention_mask=inputs["attention_mask"].to(model.device),
#     do_sample=False,  # disable sampling to test if batching affects output
# )[0]

# print(tokenizer.decode(output_sequences, skip_special_tokens=True), '\n')
# print(cnn_dailymail_tokenized['test'][i]['article'], '\n')
# print(cnn_dailymail_tokenized['test'][i]['highlights'], '\n')

# output_sequences

# o1 = model(inputs["input_ids"].to(model.device))
# o1[0]

# inputs["input_ids"].to(model.device)

# o1[0].argmax(-1)